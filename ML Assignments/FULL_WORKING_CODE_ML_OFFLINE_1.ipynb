{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1505022 [ML Offline 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_dataset_1 = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "file_name_dataset_2_train = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/adult.data\"\n",
    "file_name_dataset_2_test = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/adult.test\"\n",
    "file_name_dataset_3 = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/creditcardfraud/creditcard.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/adult.data\n",
      "age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,hours-per-week,native-country,Label,"
     ]
    }
   ],
   "source": [
    "file_name_dataset = file_name_dataset_2_train\n",
    "# file_name_dataset = file_name_dataset_3\n",
    "print(file_name_dataset)\n",
    "### NEED TO REPLACE THIS IN FILES as HEADERS [adult.data and adult.test]\n",
    "column_names_dataset_2 = [\"age\", \"workclass\", \"fnlwgt\", \"education\",\n",
    "                   \"education-num\", \"marital-status\", \"occupation\",\n",
    "                   \"relationship\", \"race\", \"sex\", \"capital-gain\",\n",
    "                   \"capital-loss\", \"hours-per-week\", \"native-country\", \"Label\"]\n",
    "for col in column_names_dataset_2:\n",
    "    print(col, end = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country   Label  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_original = pd.read_csv(file_name_dataset) \n",
    "data_frame_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: examples [numpy array, WITHOUT the labels], labels/classes [numpy array format]\n",
    "    Output: Entropy of THIS node\n",
    "    *** epsilon_small is used for log_2 operations (log2(0) may give unwanted exceptions)\n",
    "    Calculation: \n",
    "        for each label x of Labels:\n",
    "            probability[x] = x/num_examples\n",
    "        Entropy(node) = H(node) = - [ sum of probability[x]*log_2(probability[x]) ]\n",
    "\"\"\"\n",
    "def calculate_entropy(examples, labels, epsilon_small = 0.0000000000000000001): # WORKING\n",
    "    labels_unique = np.unique(labels) # obtain the unique labels of the data\n",
    "    # gives unique label_names, and counts for each unique label_names\n",
    "    label_names, label_counts = np.unique(labels,\n",
    "                                         return_counts = True) \n",
    "    label_probabilities = label_counts/sum(label_counts)\n",
    "    label_log_probabilities = np.log2((label_probabilities + epsilon_small))\n",
    "    label_products = label_probabilities * label_log_probabilities\n",
    "#     print(len(examples), \" , \", label_names , \" , \" , label_probabilities, \" , \", label_log_probabilities)\n",
    "#     print(label_products)\n",
    "    entropy = -1 * sum(label_products)\n",
    "    if entropy == 0.0:  # to not return -0.0\n",
    "        entropy = 0.0\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Finds the feature types ...\n",
    "\"\"\"\n",
    "# If num of unique vals are greater than 20, declare as continuous [if float/int]\n",
    "def check_data_type_one(unique_vals, threshold_cnt):  \n",
    "    if len(unique_vals) == 0:\n",
    "        return \"CATEGORICAL\"  # just checking\n",
    "    sample_val = unique_vals[0]\n",
    "    if isinstance(sample_val, str):\n",
    "        return \"CATEGORICAL\"\n",
    "    if isinstance(sample_val, float):\n",
    "        return \"CONTINUOUS\"\n",
    "    if len(unique_vals) > threshold_cnt:\n",
    "        return \"CONTINUOUS\"\n",
    "    else:\n",
    "        return \"CATEGORICAL\"\n",
    "# If num of unique vals are greater than 20, declare as continuous [if float/int]\n",
    "def find_data_types(data_frame, threshold=20):\n",
    "    data_type_list = []\n",
    "    for feature_test in data_frame.columns.values:\n",
    "        val_first = data_frame[feature_test]\n",
    "#         print(feature_test, \" : \", val_first, \"  :  \", type(val_first))\n",
    "        unique_vals = np.unique(data_frame[feature_test])\n",
    "#         print(unique_vals)\n",
    "        type_val = check_data_type_one(unique_vals, threshold)\n",
    "        data_type_list.append(type_val)\n",
    "        # Keep looping\n",
    "        \n",
    "    return data_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Information Gain Calculation wrt one feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: X (examples), Y (labels), feature_column(idx on X), feature_type [CONTINUOUS/CATEGORICAL],\n",
    "            use_custom_columns {optional}, custom_columns_list {optional/=}\n",
    "    Output: Information Gain wrt that feature [CATEGORICAL/BINARIZED]\n",
    "            Dictionary of information gains wrt each values of that feature [CONTINUOUS]\n",
    "    Dependency: Uses calculate_entropy() function written above\n",
    "    Calculation:\n",
    "        for each value v of examples[feature_column]:\n",
    "            Partition new_examples[v] by choosing that feature.val == v [if categorical]\n",
    "            Partition new_examples[v] by choosing that feature.val <= v [if continuous]\n",
    "            calculate entropy of new_examples, H(S_feature_val_v)\n",
    "            calculate num_examples(S_feature_val_v)/num_examples(parent_node)\n",
    "            Use formula IG = H(S{parent}) - [Sum of |S_val|/|S| * H(S_val)]\n",
    "    Treat either as continuous, or categorical [binarized data is treated as categorical]\n",
    "\"\"\"\n",
    "def calculate_information_gain(X, Y, feature_column, feature_type,\n",
    "                              use_custom_columns = False, custom_columns_list = None):  # WORKING\n",
    "    entropy_parent_node = calculate_entropy(X, Y) # entropy of parent node\n",
    "    # Either use ALL values for this column OR use custom values only\n",
    "    if ((use_custom_columns == True)) :\n",
    "        unique_vals_features = custom_columns_list\n",
    "    else:\n",
    "        unique_vals_features = np.unique(X[:, feature_column])\n",
    "#         print(\"In calculate_info_gain() ... feature_col = \", feature_column, \" , printing unique values of the feature : \", unique_vals_features)\n",
    "#         print(\"-->>DOING FOR ALL UNIQUE CONTINUOUS COLUMNS, feature-col = \", feature_column,\n",
    "#             \" , feature_type = \", feature_type, \" len_unique_feature_vals = \", len(unique_vals_features))\n",
    "    if feature_type == \"CONTINUOUS\":\n",
    "#         print(\"-->>FEATURE_COL = \", feature_column, \" .. inside if feature type == CONTINUOUS\")\n",
    "        # Partition into two sets ... x <= val and x > val\n",
    "        info_gain_dict = {}\n",
    "        for val in unique_vals_features:\n",
    "            idx_left_bool = X[:, feature_column] < float(val)\n",
    "            idx_right_bool = X[:, feature_column] >= float(val)\n",
    "            data_left = X[idx_left_bool]\n",
    "            label_left = Y[idx_left_bool]\n",
    "            data_right = X[idx_right_bool]\n",
    "            label_right = Y[idx_right_bool]\n",
    "            entropy_left = calculate_entropy(data_left, label_left)\n",
    "            entropy_right = calculate_entropy(data_right, label_right)\n",
    "            info_gain = entropy_parent_node - (\n",
    "                ((len(data_left)/len(X)) * entropy_left) + \n",
    "                ((len(data_right)/len(X)) * entropy_right)\n",
    "            )\n",
    "            info_gain_dict[val] = info_gain\n",
    "#         print(\"+++-->>Inside calculate_info_gain() [CONTINUOUS] ... dictionary-len = \", \n",
    "#               len(info_gain_dict), \" ... printing info_gain_dict .. \", info_gain_dict)\n",
    "        return info_gain_dict\n",
    "    else:# CATEGORICAL\n",
    "        ## Partition into FOR EACH FEATURE\n",
    "#         entropy_per_val = {} # empty dictionary\n",
    "        num_examples_parent = len(X)\n",
    "        cumulative_entropy = 0.0 # cumulative entropy for all features\n",
    "        if num_examples_parent == 0: # SOMEHOW comes down to this\n",
    "            print(\"-->>Inside calculateInfoGain() .. num_examples_parent = \", num_examples_parent,\n",
    "                 \" returning 0\")\n",
    "            return 0\n",
    "        for val in unique_vals_features:\n",
    "            idx_equal_to_feature = X[:, feature_column] == val\n",
    "            data_of_feature = X[idx_equal_to_feature]\n",
    "            label_of_feature = Y[idx_equal_to_feature]\n",
    "            entropy_of_feature = calculate_entropy(data_of_feature, label_of_feature)\n",
    "#             print(\"val = \", val, \" , entropy of feature = \", entropy_of_feature)\n",
    "            proportion_of_examples_in_feature = float(len(data_of_feature)) / float(num_examples_parent)\n",
    "#             print(\"proportion of examples in feature = \", proportion_of_examples_in_feature)\n",
    "            cumulative_entropy = cumulative_entropy + (proportion_of_examples_in_feature * entropy_of_feature)\n",
    "#             print(\"cumulative entropy = \", cumulative_entropy, \" parent entropy = \", entropy_parent_node)\n",
    "        #             entropy_per_val[val] = entropy_of_feature\n",
    "        # now subtract from parent's entropy to return the information gain\n",
    "        info_gain = entropy_parent_node - cumulative_entropy\n",
    "        return info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing datasets\n",
    "#### Make the last column as 'Label' and rename it to 'Label'\n",
    "#### Make any string/object datatype to integers [encoding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset 1: churn dataset\n",
    "1. Tenure is continuous\n",
    "2. MonthlyCharges is continuous\n",
    "3. TotalCharges is continuous [object type ... changed to float type] [Some missing values ... spaces, delete those rows]\n",
    "4. Drop customerID column\n",
    "\"\"\"\n",
    "def preprocess_dataset_1(df):    \n",
    "    df_copy = df.copy(deep = True)\n",
    "    df_copy.rename(columns = {'Churn' : 'Label'}, inplace=True)\n",
    "    df_copy = df_copy.drop(\"customerID\", axis = 1)  # drop customer ID\n",
    "    df_copy.drop(df_copy[df_copy.TotalCharges == ' '].index, inplace=True)  # delete rows with spaces\n",
    "    df_copy[\"TotalCharges\"] = df_copy[\"TotalCharges\"].astype(float)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of columns =  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-262-746da32ce874>:17: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if missing_val in unique_vals:  # ONLY THOSE COLUMNS THAT CONTAIN THE ' ?' mark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of columns =  15\n",
      "Train:  (32561, 14)   (32561,)   Test:  (16281, 14)   (16281,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset 2: Adult dataset\n",
    "Problems: Rows contain '?', Continuous values for age, fnlwgt, education-num, capital-gain, capital-loss, hours-per-week\n",
    "Columns that have '?' marks are : workclass, occupation , native-country [we replace with MODE]\n",
    "Col =  workclass , Mode =   Private , Percent =  69.70301894904948  %\n",
    "Col =  occupation , Mode =   Prof-specialty  , Percent =  12.714597217530175  %\n",
    "Col =  native-country , Mode =   United-States  , Percent =  89.5857006848684  %\n",
    "\"\"\"\n",
    "def preprocess_dataset_2_train(df_original):\n",
    "    df = df_original.copy(deep = True)\n",
    "    print(\"No. of columns = \", len(df.columns.values))\n",
    "    df['education-num'] = df['education-num'].astype(float)  # to make THIS column float [so that it automatically becomes continuous]\n",
    "    list_columns_with_QUESTION_mark = []\n",
    "    for col in df.columns.values:\n",
    "        unique_vals, unique_counts = np.unique(df[col].values, return_counts=True)\n",
    "        missing_val = \" ?\"\n",
    "        if missing_val in unique_vals:  # ONLY THOSE COLUMNS THAT CONTAIN THE ' ?' mark\n",
    "            list_columns_with_QUESTION_mark.append(col)\n",
    "#             idx_search = -1  # index of '?' mark\n",
    "            mode_of_unique_vals = unique_vals[np.argmax(unique_counts)]\n",
    "#             print(\"Col = \", col, \" Mode = \", mode_of_unique_vals, \" Percent = \", (unique_counts[np.argmax(unique_counts)]/len(df))*100, \" %\")\n",
    "            df[col] = np.where((df[col] == ' ?'),mode_of_unique_vals,df[col])        \n",
    "    return df, list_columns_with_QUESTION_mark\n",
    "\n",
    "data_frame, list_col_with_qstn_mark = preprocess_dataset_2_train(data_frame_original)\n",
    "data_frame_test_original = pd.read_csv(file_name_dataset_2_test)\n",
    "data_frame_test, list_col_with_qstn_mark_test = preprocess_dataset_2_train(data_frame_test_original)\n",
    "X_train = (data_frame.drop(\"Label\", axis = 1)).values\n",
    "Y_train = (data_frame[\"Label\"]).values\n",
    "X_test = (data_frame_test.drop(\"Label\", axis = 1)).values\n",
    "Y_test = (data_frame_test[\"Label\"]).values\n",
    "print(\"Train: \", X_train.shape, \" \", Y_train.shape, \"  Test: \", X_test.shape, \" \", Y_test.shape)\n",
    "# for col in list_col_with_qstn_mark:\n",
    "#     print(\"\\nCol = \", col, \" \", np.unique(data_frame[col].values, return_counts=True))\n",
    "# for col in list_col_with_qstn_mark_test:\n",
    "#     print(\"\\nCol = \", col, \" \", np.unique(data_frame_test[col].values,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset 3: Credit-Card\n",
    "All are continuous values, so we will binarize them\n",
    "Drop the 'time' column, keep only 20,000 NO/False labels and keep ALL YES/True labels\n",
    "To save time, try to binarize using 100 data points\n",
    "\"\"\"\n",
    "def preprocess_dataset_3(df, data_size = 20000):\n",
    "    df.rename(columns={'Class':'Label'}, inplace=True)  \n",
    "    # drop 'Time' column\n",
    "    df = df.drop(\"Time\", axis = 1)\n",
    "    \n",
    "    df_yes = df[df['Label'] == 1]\n",
    "    df_no  = df[df['Label'] == 0]\n",
    "    \n",
    "    indices = df_no.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=data_size)  # only keeps 'k' amount of data\n",
    "    df_no_kept = df.loc[test_indices]\n",
    "#     df_no_dropped = df.drop(test_indices)\n",
    "    # recombine the 'YES' and 'NO' samples together into a new dataframe\n",
    "    df = pd.concat([df_yes, df_no_kept])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Make the 'Label' column as separate Labels and use other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_frame = preprocess_dataset_1(data_frame_original)\n",
    "# print(data_frame.head(5))\n",
    "\n",
    "# data_frame = preprocess_dataset_3(data_frame_original)\n",
    "# print(data_frame.head(5))\n",
    "\n",
    "data_frame = preprocess_dataset_2_train(data_frame_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: dataframe\n",
    "    Output: X, Y [numpy format]\n",
    "\"\"\"\n",
    "def separate_labels_and_features(df):\n",
    "    X = df.drop(\"Label\", axis = 1)\n",
    "    Y = df[\"Label\"]\n",
    "    return X.values, Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7032, 19)\n",
      "(7032,)\n",
      "[['Female' 0 'Yes' ... 'Electronic check' 29.85 29.85]\n",
      " ['Male' 0 'No' ... 'Mailed check' 56.95 1889.5]\n",
      " ['Male' 0 'No' ... 'Mailed check' 53.85 108.15]\n",
      " ...\n",
      " ['Female' 0 'Yes' ... 'Electronic check' 29.6 346.45]\n",
      " ['Male' 1 'Yes' ... 'Mailed check' 74.4 306.6]\n",
      " ['Male' 0 'No' ... 'Bank transfer (automatic)' 105.65 6844.5]]\n"
     ]
    }
   ],
   "source": [
    "X, Y = separate_labels_and_features(data_frame)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7032, 19)   7032\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, \" \", len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All functions below are to binarize the continuous values of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: Dataset (X, Y), feature-column\n",
    "    Output: Best Information Gain wrt THIS feature column\n",
    "    Dependency: Uses calculate_information_gain(X, Y, feature_column, feature_type) function\n",
    "                which returns the info_gain [if categorical]\n",
    "                which returns dictionary of {key = split_val, value = info_gain}\n",
    "\"\"\"\n",
    "def get_best_IG_val_of_this_feature(X_train, Y_train, feature_col, custom_col_list=None,\n",
    "                                   use_custom_col_list = False):\n",
    "#     print(X_train[:, 0])\n",
    "    if use_custom_col_list == False:\n",
    "        dict_info_gain_col = calculate_information_gain(X_train, Y_train, \n",
    "                  feature_col, feature_type=\"CONTINUOUS\", use_custom_columns=False)\n",
    "    else:\n",
    "#         print(\"Inside using custom columns ... custom_col_list = \", custom_col_list)\n",
    "        dict_info_gain_col = calculate_information_gain(X_train, Y_train, \n",
    "  feature_col, feature_type=\"CONTINUOUS\", use_custom_columns=True, custom_columns_list=custom_col_list)\n",
    "#     print(\"-->Inside gt_best_IG_val_of_this_feature() .. col = \", feature_col ,\n",
    "#           \" .. dictionary len = \", len(dict_info_gain_col))\n",
    "    v = list(dict_info_gain_col.values())\n",
    "    k = list(dict_info_gain_col.keys())\n",
    "    return k[v.index(max(v))], max(v) # returns the max gain and index/split_val of that max IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    To binarize the data, instead of comparing values for EACH continuous value,\n",
    "    we will divide into 100 (or user defined number of) data points, \n",
    "    and compare with each of those values. [To make it time-efficient (Takes ~ 2mins)]\n",
    "\"\"\"\n",
    "def get_dictionary_of_best_split_values_for_each_col(X, Y, num_values_of_custom_cols = 100,\n",
    "                        use_custom_col_list = False, custom_cols = None, use_custom_end_points = False,\n",
    "                                                    which_cols_to_use_endpoints = None):\n",
    "    _, ncol = X.shape\n",
    "    num_cols_to_do = np.arange(ncol)\n",
    "    if use_custom_col_list == True:\n",
    "        num_cols_to_do = custom_cols\n",
    "    dict_best_ig_feature_split_values = {}  # Dictionary to store {feature_col: split_val, max_IG} that has max IG wrt that feature column\n",
    "#     for col_feature in num_cols_to_do:  # EITHER ONLY DO FOR SELECTED COLUMNS/ OR FOR ALL COLUMNS\n",
    "    for idx_col in range(len(num_cols_to_do)):\n",
    "        col_feature = num_cols_to_do[idx_col]\n",
    "        unique_vals_of_this_feature = np.unique(X[:, col_feature])\n",
    "        left_range = int(np.ceil(min(unique_vals_of_this_feature)))\n",
    "        right_range = int(np.floor(max(unique_vals_of_this_feature))) \n",
    "        cols_custom = np.linspace(left_range, right_range, num_values_of_custom_cols)\n",
    "        if use_custom_end_points == True:\n",
    "            if which_cols_to_use_endpoints is not None:\n",
    "                split_val_for_max_IG, max_IG = get_best_IG_val_of_this_feature(X, Y, col_feature, \n",
    "                                                       cols_custom, use_custom_col_list=True)\n",
    "            else:\n",
    "                if which_cols_to_use_endpoints[idx_col] == True:\n",
    "                    split_val_for_max_IG, max_IG = get_best_IG_val_of_this_feature(X, Y, col_feature, \n",
    "                               cols_custom, use_custom_col_list=True)\n",
    "        else:\n",
    "            # Use ALL values for continuous ....\n",
    "            split_val_for_max_IG, max_IG = get_best_IG_val_of_this_feature(X, Y, col_feature)\n",
    "        dict_best_ig_feature_split_values[col_feature] = split_val_for_max_IG, max_IG\n",
    "        print(\"Dictionary: Done for Column = \", col_feature, \" split-val = \", split_val_for_max_IG, \" Max IG = \", max_IG)\n",
    "    return dict_best_ig_feature_split_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Binarizes the dataset per column wrt one feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Actually binarizes the Data wrt the above dictionary found.\n",
    "\"\"\"\n",
    "def binarize_dataset(X, dict_split_values_and_max_IG_per_col):\n",
    "#     v = list(dict_split_values_and_max_IG_per_col.values())\n",
    "    X_bin = X\n",
    "    keys_list = list(dict_split_values_and_max_IG_per_col.keys())\n",
    "    features_column = {}\n",
    "#     print(keys_list)\n",
    "    for col in keys_list:\n",
    "        # dictionary[col][0] gives split-value and dictionary[col][1] gives max ig\n",
    "        split_value_of_col = dict_split_values_and_max_IG_per_col[col][0]\n",
    "        binarized_data_this_col = X[:, col] < split_value_of_col\n",
    "        X_bin[:, col] = binarized_data_this_col\n",
    "        features_column[col] = \"BINARIZED\"\n",
    "    \n",
    "    return X_bin, features_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_features_list(features_list_binarized, features_list_previous_arr):\n",
    "    features_list_new = {}\n",
    "    for itr in range(len(features_list_previous_arr)):\n",
    "        features_list_new[itr] = features_list_previous_arr[itr]\n",
    "    for key in list(features_list_binarized.keys()):\n",
    "        features_list_new[key] = features_list_binarized[key]\n",
    "    return list(features_list_new.values()) # returns as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Label encoding of labels/classes/Y ...\n",
    "\"\"\"\n",
    "def label_encode_labels(Y):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(Y)\n",
    "    Y= le.transform(Y)\n",
    "    return Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Label encoding of examples/X .... \n",
    "\"\"\"\n",
    "def label_encode_data(X, feature_types):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in range(len(feature_types)):\n",
    "        if feature_types[i] == \"CATEGORICAL\":\n",
    "            le.fit(X[:, i])\n",
    "            X[:, i] = le.transform(X[:, i])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CONTINUOUS', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CONTINUOUS', 'CONTINUOUS'] \n",
      " 19\n",
      "[4, 17, 18]\n",
      "================ BEFORE ======================\n",
      "{4: (17.494949494949495, 0.07579675305132505), 17: (27.0, 0.03823985984108347), 18: (369.1010101010101, 0.0340171106893129)}\n",
      "-------------------- AFTER -------------------------\n",
      "['CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'BINARIZED', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'BINARIZED', 'BINARIZED']\n"
     ]
    }
   ],
   "source": [
    "df_2 = data_frame.drop(data_frame.columns.values[-1], axis=1)\n",
    "# print(df_2.head(2))\n",
    "feature_types_before = find_data_types(df_2)\n",
    "print(feature_types_before, \"\\n\", len(feature_types_before))\n",
    "indices_continous = []\n",
    "for i in range(len(feature_types_before)):  ## ONLY BINARIZE THE CONTINUOUS FEATURES ... [float/num unique values > 15]\n",
    "    if feature_types_before[i] == \"CONTINUOUS\":\n",
    "        indices_continous.append(i)\n",
    "print(indices_continous)\n",
    "X, Y = separate_labels_and_features(data_frame)\n",
    "print(\"================ BEFORE ======================\")\n",
    "# dict_test = get_dictionary_of_best_split_values_for_each_col(X, Y, 100,\n",
    "#                                         use_custom_col_list=False)  # FOR ALL SPLIT-VALUES CONSIDERATION\n",
    "### CONSIDERS ONLY 100 data points from left-most, to right-most and splits accordingly \n",
    "## [not much worse wrt dataset 3 accuracy, however a strong improvement in time]\n",
    "## This method takes ~5s, whereas all unique values of one column takes around 1hr, 20mins [TOO MUCH !!]\n",
    "dict_test = get_dictionary_of_best_split_values_for_each_col(X, Y, 100,\n",
    "                                        use_custom_col_list=True, custom_cols=indices_continous)\n",
    "\n",
    "print(dict_test)\n",
    "print(\"-------------------- AFTER -------------------------\")\n",
    "X_binarized, features_col_new = binarize_dataset(X, dict_test)\n",
    "features_col_new = get_new_features_list(features_col_new, feature_types_before)\n",
    "print(features_col_new)\n",
    "X_binarized = label_encode_data(X_binarized, features_col_new)\n",
    "Y = label_encode_labels(Y)\n",
    "# print(X_binarized)\n",
    "# Pass this binarized data into the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONTINUOUS', 'CATEGORICAL', 'CONTINUOUS', 'CATEGORICAL', 'CONTINUOUS', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CONTINUOUS', 'CONTINUOUS', 'CONTINUOUS', 'CATEGORICAL'] \n",
      " 14\n",
      "age   CONTINUOUS\n",
      "workclass   CATEGORICAL\n",
      "fnlwgt   CONTINUOUS\n",
      "education   CATEGORICAL\n",
      "education-num   CONTINUOUS\n",
      "marital-status   CATEGORICAL\n",
      "occupation   CATEGORICAL\n",
      "relationship   CATEGORICAL\n",
      "race   CATEGORICAL\n",
      "sex   CATEGORICAL\n",
      "capital-gain   CONTINUOUS\n",
      "capital-loss   CONTINUOUS\n",
      "hours-per-week   CONTINUOUS\n",
      "native-country   CATEGORICAL\n",
      "[0, 2, 4, 10, 11, 12]\n",
      "================ BEFORE ======================\n",
      "Dictionary: Done for Column =  0  split-val =  28  Max IG =  0.0737162839458736\n",
      "Dictionary: Done for Column =  4  split-val =  13.0  Max IG =  0.07069368427626688\n",
      "Dictionary: Done for Column =  10  split-val =  7298  Max IG =  0.08695927355015609\n",
      "Dictionary: Done for Column =  11  split-val =  1825  Max IG =  0.023190335847712906\n",
      "Dictionary: Done for Column =  12  split-val =  42  Max IG =  0.040778829211068346\n",
      "{0: (28, 0.0737162839458736), 4: (13.0, 0.07069368427626688), 10: (7298, 0.08695927355015609), 11: (1825, 0.023190335847712906), 12: (42, 0.040778829211068346)}\n",
      "-------------------- AFTER -------------------------\n",
      "['BINARIZED', 'CATEGORICAL', 'CONTINUOUS', 'CATEGORICAL', 'BINARIZED', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'BINARIZED', 'BINARIZED', 'BINARIZED', 'CATEGORICAL']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset 2:\n",
    "[Col, #Unique vals, #All vals]\n",
    "[ 0 ,  73 ,  32561 ] [ 2 ,  21648 ,  32561 ] [ 4 ,  16 ,  32561 ] \n",
    "[ 10 ,  119 ,  32561 ] [ 11 ,  92 ,  32561 ] [ 12 ,  94 ,  32561 ]\n",
    "col=2 NEEDS to have custom data points try, otherwise will take too much time !!\n",
    "\"\"\"\n",
    "df_2 = data_frame.drop(data_frame.columns.values[-1], axis=1)\n",
    "feature_types_before = find_data_types(df_2)\n",
    "print(feature_types_before, \"\\n\", len(feature_types_before))\n",
    "indices_continous = []\n",
    "for i in range(len(feature_types_before)):  ## ONLY BINARIZE THE CONTINUOUS FEATURES ... [float/num unique values > 15]\n",
    "    print(df_2.columns.values[i], \" \", feature_types_before[i])\n",
    "    if feature_types_before[i] == \"CONTINUOUS\":\n",
    "        indices_continous.append(i)\n",
    "print(indices_continous)\n",
    "print(\"================ BEFORE ======================\")\n",
    "indices_continous.remove(2) # REMOVE \"fnwlgt\" column [will discretize it later]\n",
    "flag_will_use_custom = [False, True, False, False, False, False]\n",
    "dict_binarized_values = get_dictionary_of_best_split_values_for_each_col(X, Y, 100,\n",
    "                            use_custom_col_list=True, custom_cols=indices_continous,\n",
    "                            use_custom_end_points=False)\n",
    "\n",
    "print(dict_binarized_values)\n",
    "print(\"-------------------- AFTER -------------------------\")\n",
    "X_binarized, features_col_new = binarize_dataset(X_train, dict_binarized_values) # binarize\n",
    "features_col_new = get_new_features_list(features_col_new, feature_types_before)\n",
    "X_train = label_encode_data(X_binarized, features_col_new) # label encoding [EXAMPLES, Training data]\n",
    "enc = sklearn.preprocessing.KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans') # 10 bins\n",
    "# strategies: quantile, kmeans, uniform [encod = 'ordinal' FOR integer]\n",
    "# X_train[:, 2] = (enc.fit_transform(X_train[:, 2].reshape(-1, 1))).reshape(-1, )\n",
    "Y_train = label_encode_labels(Y_train) # label encoding [LABELS, Training data]\n",
    "X_test, features_col_new_test = binarize_dataset(X_test, dict_binarized_values) # binarize\n",
    "X_test = label_encode_data(X_test, features_col_new) # label encoding [EXAMPLES, TEST DATA]\n",
    "# X_test[:, 2] = (enc.fit_transform(X_test[:, 2].reshape(-1, 1))).reshape(-1, )  # DISCRETIZATION\n",
    "Y_test = label_encode_labels(Y_test) # label encoding [LABELS, TEST DATA]\n",
    "print(features_col_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Column 2 issue ..\n",
    "# col_2_unique_vals = X_train[:, 2]\n",
    "# print(col_2_unique_vals.shape)\n",
    "# print(col_2_unique_vals)\n",
    "# col_2_unique_vals = col_2_unique_vals.reshape(-1, 1)\n",
    "# # sklearn.preprocessing.KBinsDiscretizer(n_bins=5, encode='onehot', strategy='quantile')\n",
    "\n",
    "# enc = sklearn.preprocessing.KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')\n",
    "# col_2_binned = enc.fit_transform(col_2_unique_vals)\n",
    "# print(col_2_unique_vals)\n",
    "# print(col_2_binned, \" \", col_2_binned.shape, np.unique(col_2_binned))\n",
    "# col_2_binned = col_2_binned.reshape(-1, )\n",
    "# print(col_2_binned, \" \", col_2_binned.shape, np.unique(col_2_binned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Compute new feature types ... [Binarized/Categorical/Continuous]\n",
    "\"\"\"\n",
    "def obtain_new_feature_types(feature_types_prev, dict_feature_types):  # WORKING\n",
    "    keys = list(dict_feature_types.keys())\n",
    "    feature_types_latest = feature_types_prev # Copy previous feature types ...\n",
    "    for i in keys:\n",
    "        feature_types_latest[i] = dict_feature_types[i]\n",
    "    return feature_types_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BINARIZED', 'CATEGORICAL', 'CONTINUOUS', 'CATEGORICAL', 'BINARIZED', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'BINARIZED', 'BINARIZED', 'BINARIZED', 'CATEGORICAL']\n"
     ]
    }
   ],
   "source": [
    "# feature_types = obtain_new_feature_types(feature_types_before, features_col_new)\n",
    "# print(feature_types, \" \", len(feature_types))\n",
    "feature_types = features_col_new\n",
    "print(feature_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to numpy and train_test_split using scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_binarized, Y, test_size=0.2, random_state=8)\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X_binarized, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5625, 19)   (5625,)   (1407, 19)   (1407,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, \" \", Y_train.shape , \" \", X_test.shape, \" \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form decision tree bulding algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Returns the majority of the label\n",
    "\"\"\"\n",
    "def return_majority_label(labels):\n",
    "    # obtains unique labels AND also the counts of those unique labels\n",
    "    label_names, label_counts = np.unique(labels, return_counts = True)\n",
    "    index_max = label_counts.argmax()\n",
    "    labels_with_max_count = label_names[index_max]\n",
    "    return labels_with_max_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Class DecisionTreeNode to store the decision trees/subtrees nodes.\n",
    "    CAN'T HAVE CONTINUOUS DATA [ONLY BINARIZED/CATEGORICAL]\n",
    "\"\"\"\n",
    "class DTreeNode:\n",
    "    def __init__(self):\n",
    "        # self.sub_trees = []  # List of subtrees/children\n",
    "        # self.feature_values = [] # list of feature values of children [To ask question use indices]\n",
    "        self.children = {} # [feature_val: DTreeNode]\n",
    "        self.feature_col = -1 # which feature column THIS node contains as a question\n",
    "        self.is_leaf_node = False # by default, shouldn't be a leaf\n",
    "        self.classification = \"NONE\" # also consider as the plurality value\n",
    "        \n",
    "    def printTree(self, spaces_num = 0):\n",
    "        node = self\n",
    "        if node.is_leaf_node == True:  # Only print the classification column/feature\n",
    "            print(\" \" * spaces_num, \"Lab(\", node.classification, \")\")\n",
    "            return\n",
    "        else:  # Print the question\n",
    "            # print(\" \" * spaces_num, node.feature_col, \" \", node.get_comparison_mark(), \" \", node.feature_val)\n",
    "            print(\"\\n\", (\"    \" * spaces_num), \"Q(\", node.feature_col, \")\")\n",
    "        spaces_num = spaces_num + 1\n",
    "        for key in list(self.children.keys()):\n",
    "            print(\"  \"*spaces_num, \" == \", key)\n",
    "            node = self.children[key]\n",
    "            node.printTree(spaces_num)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Prints various metrics.\n",
    "\"\"\"\n",
    "def print_metrics(Y_true, Y_predicted):\n",
    "    # print(len(Y_pred))\n",
    "    TN, FP, FN, TP = confusion_matrix(Y_true, Y_predicted).ravel()\n",
    "    accuracy = (TP + TN)/(TP+TN+FP+FN)\n",
    "    recall = (TP)/(TP + FN)\n",
    "    specificity = (TN)/(TN + FP)\n",
    "    precision = (TP)/(TP + FP)\n",
    "    false_discovery_rate = (FP)/(TP + FP)\n",
    "    f1_score = 2*((precision * recall) / (precision + recall))\n",
    "    print(\"TN = \", TN, \" FP = \", FP, \" FN = \", FN, \" TP = \", TP)\n",
    "    print(\"Accuracy = \", accuracy*100, \"%\")\n",
    "    print(\"TPR = Sensitivity = Recall = \", recall*100, \"%\")\n",
    "    print(\"TNR = Specificity = \", specificity*100, \"%\")\n",
    "    print(\"Precision = PPV = Positive Predictive Value = \", precision*100, \"%\")\n",
    "    print(\"FDR = False Discovery Rate = \", false_discovery_rate*100, \"%\")\n",
    "    print(\"F1 Score = \", f1_score*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Can't have CONTINUOUS data [Will be binarized first]\n",
    "    Decision Tree Classifier\n",
    "\"\"\"    \n",
    "class DTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.d_tree_root = DTreeNode()\n",
    "        self.max_depth = 5\n",
    "        self.print_termination_msg = True\n",
    "    \n",
    "    def form_a_leaf_node(self, labels):\n",
    "        leaf_node = DTreeNode()\n",
    "        leaf_node.is_leaf_node = True\n",
    "        leaf_node.classification = return_majority_label(labels)\n",
    "        return leaf_node\n",
    "    ##### Can't have CONTINUOUS data [ONLY CATEGORICAL/BINARIZED DATA IS ALLOWED !!]\n",
    "    def recursive_fit(self, X, Y, X_parent, Y_parent, current_depth, max_depth):\n",
    "        ## 1. If current-depth == max-depth [Base Case]\n",
    "        if current_depth == max_depth:\n",
    "            return self.form_a_leaf_node(Y) # return the leaf node with majority of the current labels\n",
    "        ## 2. If EXACTLY one label ... return that\n",
    "        if (len(np.unique(Y)) == 1):\n",
    "            return self.form_a_leaf_node(Y) # return the leaf node with majority of the current labels\n",
    "        ## 2_2. If no more examples, return max label of parent\n",
    "        if len(X) == 0:\n",
    "            return self.form_a_leaf_node(Y_parent)\n",
    "        \n",
    "        current_depth = current_depth + 1  # increment current_depth variable\n",
    "        val_max_IG = col_max_IG = -1\n",
    "        for col in range(0, X.shape[1]):  ## for each number of columns/features\n",
    "            ig_this_col = calculate_information_gain(X, Y, col, \"CATEGORICAL\")\n",
    "#             print(\"col = \", col, \" ig_col = \", ig_current_col)\n",
    "            if ig_this_col > val_max_IG:\n",
    "                val_max_IG = ig_this_col\n",
    "                col_max_IG = col\n",
    "        \n",
    "        ## 3. Max IG obtained is 0 [no more examples/features]\n",
    "        if val_max_IG == 0:\n",
    "            return self.form_a_leaf_node(Y)\n",
    "        \n",
    "        d_tree_node = DTreeNode()\n",
    "        d_tree_node.classification = return_majority_label(Y)  ### Saves the pluarility value of THIS node [used in prediction]\n",
    "        ## -> Recursion\n",
    "        for val_of_this_feature in np.unique(X[:, col_max_IG]):\n",
    "            index_child = (X[:, col_max_IG] == val_of_this_feature)\n",
    "            X_child = X[index_child]\n",
    "            Y_child = Y[index_child]\n",
    "\n",
    "            d_tree_node.is_leaf_node = False  ### is an internal node\n",
    "            d_tree_node.feature_col = col_max_IG  ### question is to be asked on this column/feature\n",
    "            # d_tree_node.feature_values.append(val_of_this_feature)\n",
    "#             if (X_child.shape[0] == 0): # no more examples [DON'T return as other values of list will be empty]\n",
    "#                 d_tree_node.children.children[val_of_this_feature] = self.form_a_leaf_node(Y)  # return parent's max plurality value\n",
    "#                 # d_tree_node.sub_trees.append(self.form_a_leaf_node(labels))\n",
    "#             else:\n",
    "            ##### DICTIONARY {key = feature_val, value = sub-tree}\n",
    "            sub_tree = self.recursive_fit(X_child, Y_child, X, Y, current_depth, max_depth)\n",
    "            d_tree_node.children[val_of_this_feature] = sub_tree\n",
    "                # d_tree_node.sub_trees.append(sub_tree)\n",
    "        ### END OF FOR LOOP, return d_tree_node\n",
    "        return d_tree_node\n",
    "    \n",
    "    def printTree(self):\n",
    "        print(\"-->Inside printTree\")\n",
    "        self.d_tree_root.printTree()\n",
    "\n",
    "    def fit(self, examples, labels):\n",
    "        dt_node = self.recursive_fit(examples, labels, examples, labels,\n",
    "                                     current_depth = 0, max_depth=self.max_depth)\n",
    "        self.d_tree_root = dt_node\n",
    "        if self.print_termination_msg == True:\n",
    "            print(\"Fit done for, max-depth = \", self.max_depth)\n",
    "        \n",
    "    def predict_one_example(self, example_to_predict):\n",
    "        self.d_tree_root_backup = self.d_tree_root\n",
    "        root = self.d_tree_root\n",
    "        if root == None:\n",
    "            raise Exception('Root of the Decision Tree is null !! [In predict()]')\n",
    "        while root is not None:\n",
    "            if root.is_leaf_node == True: # classify ... since, it is the leaf\n",
    "                self.d_tree_root = self.d_tree_root_backup\n",
    "                return root.classification\n",
    "            else:\n",
    "                col_to_process = root.feature_col\n",
    "                val_present_in_example = example_to_predict[col_to_process]\n",
    "                bool_found = False\n",
    "#                 print(\"In col = \", col_to_process, \" len root.children = \", len(root.children))\n",
    "                # for idx in range(len(root.feature_values)):\n",
    "                keys_feature_vals = list(root.children.keys())\n",
    "                for feature_val in keys_feature_vals:\n",
    "                    if example_to_predict[root.feature_col] == feature_val:\n",
    "                        root = root.children[feature_val]\n",
    "                        bool_found = True\n",
    "                        break\n",
    "                    \n",
    "                if bool_found == False: ### SHOULD RETURN PARENT's PLURALITY VALUE\n",
    "                    self.d_tree_root = self.d_tree_root_backup\n",
    "                    return root.classification # PARENT's plurality value\n",
    "                    #raise Exception(\"The value of \", val_present_in_example, \" doesn't exist for col_idx = \", col_to_process)\n",
    "                \n",
    "    def predict(self, examples_test):\n",
    "        labels = []\n",
    "        for example in examples_test:\n",
    "            yp1 = self.predict_one_example(example)\n",
    "            labels.append(yp1)\n",
    "        return labels\n",
    "        # return TN, FP, FN, TP, accuracy, recall, specificity, precision, false_discovery_rate, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit done for, max-depth =  10000\n"
     ]
    }
   ],
   "source": [
    "d_tree_custom_DT = DTreeClassifier()\n",
    "# d_tree.fit(X_train[0:100], Y_train[0:100], max_depth = 10000)\n",
    "d_tree_custom_DT.max_depth = 10000 # here, depth-max is not to be considered ...\n",
    "d_tree_custom_DT.fit(X_train, Y_train)\n",
    "# 5 s for dataset 1\n",
    "# 12 s for dataset 2\n",
    "# 10 s for dataset 3 [with 20k samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->Inside printTree\n",
      "\n",
      "  Q( 14 )\n",
      "    ==  0\n",
      "  Lab( 0 )\n",
      "    ==  1\n",
      "  Lab( 0 )\n",
      "    ==  2\n",
      "  Lab( 0 )\n"
     ]
    }
   ],
   "source": [
    "d_tree_custom_DT.printTree()\n",
    "# print(np.unique(Y_train, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Training ----------------\n",
      "TN =  23995  FP =  725  FN =  1826  TP =  6015\n",
      "Accuracy =  92.16547403335278 %\n",
      "TPR = Sensitivity = Recall =  76.71215406198189 %\n",
      "TNR = Specificity =  97.06715210355988 %\n",
      "Precision = PPV = Positive Predictive Value =  89.2433234421365 %\n",
      "FDR = False Discovery Rate =  10.756676557863502 %\n",
      "F1 Score =  82.50462931211851 %\n",
      "------------- Testing ----------------\n",
      "TN =  11359  FP =  1076  FN =  1617  TP =  2229\n",
      "Accuracy =  83.45924697500153 %\n",
      "TPR = Sensitivity = Recall =  57.956318252730114 %\n",
      "TNR = Specificity =  91.3470044229996 %\n",
      "Precision = PPV = Positive Predictive Value =  67.44326777609683 %\n",
      "FDR = False Discovery Rate =  32.556732223903175 %\n",
      "F1 Score =  62.34093133827437 %\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = d_tree_custom_DT.predict(X_train)\n",
    "print(\"------------- Training ----------------\")\n",
    "print_metrics(Y_train, Y_pred_train)\n",
    "Y_pred_test = d_tree_custom_DT.predict(X_test)\n",
    "print(\"------------- Testing ----------------\")\n",
    "print_metrics(Y_test, Y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(criterion='entropy')\n",
    "dtc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== TRAINING =================\n",
      "TN =  23542  FP =  2466  FN =  1178  TP =  5375\n",
      "Accuracy =  88.80869752157489 %\n",
      "TPR = Sensitivity = Recall =  82.02350068670837 %\n",
      "TNR = Specificity =  90.51830206090433 %\n",
      "Precision = PPV = Positive Predictive Value =  68.54992985588572 %\n",
      "FDR = False Discovery Rate =  31.45007014411427 %\n",
      "F1 Score =  74.68389606780603 %\n",
      "+++++++++++++++ TESTING ++++++++++++++++++\n",
      "TN =  11079  FP =  1865  FN =  1356  TP =  1981\n",
      "Accuracy =  80.2162029359376 %\n",
      "TPR = Sensitivity = Recall =  59.36469883128559 %\n",
      "TNR = Specificity =  85.59177997527813 %\n",
      "Precision = PPV = Positive Predictive Value =  51.508060322412895 %\n",
      "FDR = False Discovery Rate =  48.491939677587105 %\n",
      "F1 Score =  55.15801197271336 %\n"
     ]
    }
   ],
   "source": [
    "Y_p_train = dtc.predict(X_train)\n",
    "Y_p_test = dtc.predict(X_test)\n",
    "print(\"=============== TRAINING =================\")\n",
    "print_metrics(Y_p_train, Y_train)\n",
    "print(\"+++++++++++++++ TESTING ++++++++++++++++++\")\n",
    "print_metrics(Y_p_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, mode=\"DECISION_STUMP\"):\n",
    "        self.mode = mode\n",
    "    \n",
    "    # factory method to produce classifier\n",
    "    def produce_classifier_generic(self): # for now, default is decision stump\n",
    "        if self.mode == \"DECISION_STUMP\":\n",
    "            classifier = DTreeClassifier()\n",
    "            classifier.print_termination_msg = False\n",
    "            classifier.max_depth = 1 # decision stump\n",
    "        else:  # Other modes depending on the 'mode' variable\n",
    "            classifier = None\n",
    "        return classifier\n",
    "    \n",
    "    # def train_generic(self, data_examples, data_labels):\n",
    "        # classifier = self.produce_classifier_generic(mode=\"DECISION_STUMP\")\n",
    "        # classifier.fit(data_examples, data_labels)\n",
    "        # return classifier\n",
    "    \n",
    "    def normalize(self, w):\n",
    "        if sum(w) == 0:\n",
    "            return w\n",
    "        else:\n",
    "            return w/sum(w)\n",
    "    \n",
    "    def resample(self, examples, labels, w):\n",
    "        # returns resampled data and labels according to weights\n",
    "        w = self.normalize(w)\n",
    "        resampled_data_size = len(labels)\n",
    "        temp_arr = np.arange(resampled_data_size)\n",
    "        indices_sampled = np.random.choice(temp_arr, len(temp_arr), p=w)  # N resampled data points for now\n",
    "        examples_sampled = []\n",
    "        labels_sampled = []\n",
    "#         print(\"Inside resample ... indices_sampled = \", indices_sampled)\n",
    "        for idx in indices_sampled:  # probably a better method exists using numpy\n",
    "            examples_sampled.append(examples[idx])\n",
    "            labels_sampled.append(labels[idx])\n",
    "        \n",
    "        return np.asarray(examples_sampled), np.asarray(labels_sampled)\n",
    "    \n",
    "    # Return the label with the maximum weight of classifier\n",
    "    def predict(self, example):\n",
    "        # h: contains the K number of classifiers\n",
    "        # z: contains the weights of each classifier\n",
    "        labels_hypothesis = np.zeros(self.K) # Initially set to zeros\n",
    "        unique_labs_weights = {}\n",
    "        for k in range(self.K):\n",
    "            labels_hypothesis[k] = self.h[k].predict_one_example(example)            \n",
    "        for unique_labels in (np.unique(labels_hypothesis)):\n",
    "            unique_labs_weights[unique_labels] = 0.0 # initially set as 0 weight\n",
    "        for k in range(self.K):\n",
    "            unique_labs_weights[labels_hypothesis[k]] = unique_labs_weights[labels_hypothesis[k]] + self.z[k] \n",
    "        \n",
    "        v = list(unique_labs_weights.values())\n",
    "        k = list(unique_labs_weights.keys())\n",
    "        return k[v.index(max(v))]  # return the unique label with the MAX weight\n",
    "        \n",
    "        \n",
    "    def boost(self, examples, labels, K): # K : number of classifiers        \n",
    "#         L_weak = self.produce_classifier_generic() # generic interface [in this case a decision stump]\n",
    "        # Local Variables\n",
    "        self.K = K\n",
    "        N = examples.shape[0]\n",
    "        self.w = np.full(shape=(N, ), fill_value = (1/N)) # w, a vector of 'N' weights, 1/N initially\n",
    "#         h = np.full(shape=(K, ), fill_value = 0) # h, a vector of K hypothesis\n",
    "#         z = np.full(shape=(K, ), fill_value = 0) # z, a vector of K hypothesis weights\n",
    "        self.h = {} # dictionary\n",
    "        self.z = np.zeros(shape=(K, ))\n",
    "        for k in range(K):\n",
    "#             print(\"k = \", k)\n",
    "            data_resampled, labels_resampled = self.resample(examples, labels, self.w)\n",
    "            self.h[k] = self.produce_classifier_generic()\n",
    "            # FIT on RESAMPLED data [labels are also resampled]\n",
    "            self.h[k].fit(data_resampled, labels_resampled) \n",
    "#             h[k] = L_weak\n",
    "            error = 0\n",
    "            for j in range(0, N):\n",
    "                if self.h[k].predict_one_example(examples[j]) != labels[j]: # PREDICT on INITIAL EXAMPLES\n",
    "                    error = error + self.w[j]\n",
    "#             print(\"-->>k = \", k, \", Error = \", error)\n",
    "            if error > 0.5:\n",
    "                continue\n",
    "            for j in range(0, N):\n",
    "                if self.h[k].predict_one_example(examples[j]) == labels[j]:\n",
    "                    self.w[j] = self.w[j] * (error/(1 - error))  # error CAN'T be 1 due to continue condition\n",
    "#             print(\"+++>> k = \", k, \" , w = \", w)\n",
    "            # Normalize w\n",
    "            \n",
    "            self.w = self.normalize(self.w)\n",
    "            # Error can be 0\n",
    "            if error == 0:\n",
    "                self.z[k] = 10 # max of 10^10 ?? [impossible case so, assign a large number]\n",
    "            else:\n",
    "                self.z[k] = np.log((1-error)/error)\n",
    "        \n",
    "#         return self.Weighted_Majority(h, z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Train AdaBoost K =  5 -----------------\n",
      "TN =  23495  FP =  1225  FN =  4634  TP =  3207\n",
      "Accuracy =  82.00608089432143 %\n",
      "TPR = Sensitivity = Recall =  40.90039535773498 %\n",
      "TNR = Specificity =  95.04449838187702 %\n",
      "Precision = PPV = Positive Predictive Value =  72.36010830324909 %\n",
      "FDR = False Discovery Rate =  27.6398916967509 %\n",
      "F1 Score =  52.2610608653141 %\n",
      "------------- Test AdaBoost K =  5 -----------------\n",
      "TN =  11814  FP =  621  FN =  2281  TP =  1565\n",
      "Accuracy =  82.17554204287207 %\n",
      "TPR = Sensitivity = Recall =  40.69162766510661 %\n",
      "TNR = Specificity =  95.00603136308806 %\n",
      "Precision = PPV = Positive Predictive Value =  71.59194876486734 %\n",
      "FDR = False Discovery Rate =  28.408051235132664 %\n",
      "F1 Score =  51.88992042440318 %\n",
      "------------- Train AdaBoost K =  10 -----------------\n",
      "TN =  23585  FP =  1135  FN =  4092  TP =  3749\n",
      "Accuracy =  83.94705322318111 %\n",
      "TPR = Sensitivity = Recall =  47.81277898227267 %\n",
      "TNR = Specificity =  95.40857605177993 %\n",
      "Precision = PPV = Positive Predictive Value =  76.76085176085175 %\n",
      "FDR = False Discovery Rate =  23.23914823914824 %\n",
      "F1 Score =  58.923379174852656 %\n",
      "------------- Test AdaBoost K =  10 -----------------\n",
      "TN =  11863  FP =  572  FN =  2023  TP =  1823\n",
      "Accuracy =  84.06117560346415 %\n",
      "TPR = Sensitivity = Recall =  47.399895995839834 %\n",
      "TNR = Specificity =  95.40008041817451 %\n",
      "Precision = PPV = Positive Predictive Value =  76.1169102296451 %\n",
      "FDR = False Discovery Rate =  23.883089770354907 %\n",
      "F1 Score =  58.42012497997116 %\n",
      "------------- Train AdaBoost K =  15 -----------------\n",
      "TN =  23732  FP =  988  FN =  4194  TP =  3647\n",
      "Accuracy =  84.0852553668499 %\n",
      "TPR = Sensitivity = Recall =  46.511924499426094 %\n",
      "TNR = Specificity =  96.0032362459547 %\n",
      "Precision = PPV = Positive Predictive Value =  78.6839266450917 %\n",
      "FDR = False Discovery Rate =  21.31607335490831 %\n",
      "F1 Score =  58.464251362616224 %\n",
      "------------- Test AdaBoost K =  15 -----------------\n",
      "TN =  11934  FP =  501  FN =  2085  TP =  1761\n",
      "Accuracy =  84.11645476322093 %\n",
      "TPR = Sensitivity = Recall =  45.78783151326053 %\n",
      "TNR = Specificity =  95.97104945717733 %\n",
      "Precision = PPV = Positive Predictive Value =  77.85145888594165 %\n",
      "FDR = False Discovery Rate =  22.148541114058357 %\n",
      "F1 Score =  57.66208251473477 %\n",
      "------------- Train AdaBoost K =  20 -----------------\n",
      "TN =  23457  FP =  1263  FN =  3745  TP =  4096\n",
      "Accuracy =  84.61963698903597 %\n",
      "TPR = Sensitivity = Recall =  52.23823491901543 %\n",
      "TNR = Specificity =  94.89077669902912 %\n",
      "Precision = PPV = Positive Predictive Value =  76.43217018100393 %\n",
      "FDR = False Discovery Rate =  23.56782981899608 %\n",
      "F1 Score =  62.06060606060606 %\n",
      "------------- Test AdaBoost K =  20 -----------------\n",
      "TN =  11799  FP =  636  FN =  1853  TP =  1993\n",
      "Accuracy =  84.71224126282169 %\n",
      "TPR = Sensitivity = Recall =  51.82007280291212 %\n",
      "TNR = Specificity =  94.8854041013269 %\n",
      "Precision = PPV = Positive Predictive Value =  75.80829212628376 %\n",
      "FDR = False Discovery Rate =  24.191707873716243 %\n",
      "F1 Score =  61.55984555984557 %\n",
      "Total time taken =  55\n"
     ]
    }
   ],
   "source": [
    "k_list = [5, 10, 15, 20]\n",
    "import datetime\n",
    "t_before = datetime.datetime.now()\n",
    "for K in k_list:\n",
    "    adaBoost = AdaBoost()\n",
    "    adaBoost.boost(X_train, Y_train, K)\n",
    "    Y_pred_train_adaBoost = []\n",
    "    for ex in X_train:  # predict train dataset\n",
    "        Y_pred_train_adaBoost.append(adaBoost.predict(ex))\n",
    "    Y_pred_test_adaBoost = []\n",
    "    for ex in X_test:  # predict test dataset\n",
    "        Y_pred_test_adaBoost.append(adaBoost.predict(ex))\n",
    "    print(\"------------- Train AdaBoost K = \", K, \"-----------------\")\n",
    "    print_metrics(Y_train, Y_pred_train_adaBoost)\n",
    "    print(\"------------- Test AdaBoost K = \", K, \"-----------------\")\n",
    "    print_metrics(Y_test, Y_pred_test_adaBoost)\n",
    "    \n",
    "t_after = datetime.datetime.now()\n",
    "del_t = t_after - t_before\n",
    "print(\"Total time taken = \", del_t.seconds, \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
