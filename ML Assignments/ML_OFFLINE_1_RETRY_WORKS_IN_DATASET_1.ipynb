{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1505022 [ML Offline 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_dataset_1 = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "# file_name_dataset = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/Offline/PreprocessedSmallDatasets/evade.csv\"\n",
    "\n",
    "# file_name_dataset_2 = \"\"\n",
    "file_name_dataset_3 = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/creditcardfraud/creditcard.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\n"
     ]
    }
   ],
   "source": [
    "# file_name_dataset = file_name_dataset_3\n",
    "file_name_dataset = file_name_dataset_1\n",
    "print(file_name_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
       "0  No phone service             DSL             No  ...               No   \n",
       "1                No             DSL            Yes  ...              Yes   \n",
       "2                No             DSL            Yes  ...               No   \n",
       "3  No phone service             DSL            Yes  ...              Yes   \n",
       "4                No     Fiber optic             No  ...               No   \n",
       "\n",
       "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
       "0          No          No              No  Month-to-month              Yes   \n",
       "1          No          No              No        One year               No   \n",
       "2          No          No              No  Month-to-month              Yes   \n",
       "3         Yes          No              No        One year               No   \n",
       "4          No          No              No  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
       "0           Electronic check          29.85         29.85    No  \n",
       "1               Mailed check          56.95        1889.5    No  \n",
       "2               Mailed check          53.85        108.15   Yes  \n",
       "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
       "4           Electronic check          70.70        151.65   Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_original = pd.read_csv(file_name_dataset) \n",
    "data_frame_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7043, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Syntax:\n",
    "c = np.linspace(0, 1, 6)   # start, end, num-points\n",
    "numpy.arange(10) ==> creates 10 values of integers from 0 to 9\n",
    "np.full(shape=10, fill_value=3, dtype=np.int) ==> array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: examples [numpy array, WITHOUT the labels], labels/classes [numpy array format]\n",
    "    Output: Entropy of THIS node\n",
    "    *** epsilon_small is used for log_2 operations (log2(0) may give unwanted exceptions)\n",
    "    Calculation: \n",
    "        for each label x of Labels:\n",
    "            probability[x] = x/num_examples\n",
    "        Entropy(node) = H(node) = - [ sum of probability[x]*log_2(probability[x]) ]\n",
    "\"\"\"\n",
    "def calculate_entropy(examples, labels, epsilon_small = 0.0000000000000000001): # WORKING\n",
    "    labels_unique = np.unique(labels) # obtain the unique labels of the data\n",
    "    # gives unique label_names, and counts for each unique label_names\n",
    "    label_names, label_counts = np.unique(labels,\n",
    "                                         return_counts = True) \n",
    "    label_probabilities = label_counts/sum(label_counts)\n",
    "    label_log_probabilities = np.log2((label_probabilities + epsilon_small))\n",
    "    label_products = label_probabilities * label_log_probabilities\n",
    "#     print(len(examples), \" , \", label_names , \" , \" , label_probabilities, \" , \", label_log_probabilities)\n",
    "#     print(label_products)\n",
    "    entropy = -1 * sum(label_products)\n",
    "    if entropy == 0.0:  # to not return -0.0\n",
    "        entropy = 0.0\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Finds the feature types ...\n",
    "\"\"\"\n",
    "# If num of unique vals are greater than 20, declare as continuous [if float/int]\n",
    "def check_data_type_one(unique_vals, threshold_cnt = 20):  \n",
    "    if len(unique_vals) == 0:\n",
    "        return \"CATEGORICAL\"  # just checking\n",
    "    sample_val = unique_vals[0]\n",
    "    if isinstance(sample_val, str):\n",
    "        return \"CATEGORICAL\"\n",
    "    if isinstance(sample_val, float):\n",
    "        return \"CONTINUOUS\"\n",
    "    if len(unique_vals) > threshold_cnt:\n",
    "        return \"CONTINUOUS\"\n",
    "    else:\n",
    "        return \"CATEGORICAL\"\n",
    "    \n",
    "def find_data_types(data_frame):\n",
    "    data_type_list = []\n",
    "    for feature_test in data_frame.columns.values:\n",
    "        val_first = data_frame[feature_test]\n",
    "#         print(feature_test, \" : \", val_first, \"  :  \", type(val_first))\n",
    "        unique_vals = np.unique(data_frame[feature_test])\n",
    "#         print(unique_vals)\n",
    "        type_val = check_data_type_one(unique_vals)\n",
    "        data_type_list.append(type_val)\n",
    "        # Keep looping\n",
    "        \n",
    "    return data_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Information Gain Calculation wrt one feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: X (examples), Y (labels), feature_column(idx on X), feature_type [CONTINUOUS/CATEGORICAL],\n",
    "            use_custom_columns {optional}, custom_columns_list {optional/=}\n",
    "    Output: Information Gain wrt that feature [CATEGORICAL/BINARIZED]\n",
    "            Dictionary of information gains wrt each values of that feature [CONTINUOUS]\n",
    "    Dependency: Uses calculate_entropy() function written above\n",
    "    Calculation:\n",
    "        for each value v of examples[feature_column]:\n",
    "            Partition new_examples[v] by choosing that feature.val == v [if categorical]\n",
    "            Partition new_examples[v] by choosing that feature.val <= v [if continuous]\n",
    "            calculate entropy of new_examples, H(S_feature_val_v)\n",
    "            calculate num_examples(S_feature_val_v)/num_examples(parent_node)\n",
    "            Use formula IG = H(S{parent}) - [Sum of |S_val|/|S| * H(S_val)]\n",
    "    Treat either as continuous, or categorical [binarized data is treated as categorical]\n",
    "\"\"\"\n",
    "def calculate_information_gain(X, Y, feature_column, feature_type,\n",
    "                              use_custom_columns = False, custom_columns_list = None):  # WORKING\n",
    "    entropy_parent_node = calculate_entropy(X, Y) # entropy of parent node\n",
    "    if ((use_custom_columns == True)) :\n",
    "        unique_vals_features = custom_columns_list\n",
    "    else:\n",
    "        unique_vals_features = np.unique(X[:, feature_column])\n",
    "#         print(\"In calculate_info_gain() ... feature_col = \", feature_column, \" , printing unique values of the feature : \", unique_vals_features)\n",
    "#         print(\"-->>DOING FOR ALL UNIQUE CONTINUOUS COLUMNS, feature-col = \", feature_column,\n",
    "#             \" , feature_type = \", feature_type, \" len_unique_feature_vals = \", len(unique_vals_features))\n",
    "    if feature_type == \"CONTINUOUS\":\n",
    "#         print(\"-->>FEATURE_COL = \", feature_column, \" .. inside if feature type == CONTINUOUS\")\n",
    "        # Partition into two sets ... x <= val and x > val\n",
    "        info_gain_dict = {}\n",
    "        for val in unique_vals_features:\n",
    "            idx_left_bool = X[:, feature_column] < float(val)\n",
    "            idx_right_bool = X[:, feature_column] >= float(val)\n",
    "            data_left = X[idx_left_bool]\n",
    "            label_left = Y[idx_left_bool]\n",
    "            data_right = X[idx_right_bool]\n",
    "            label_right = Y[idx_right_bool]\n",
    "            entropy_left = calculate_entropy(data_left, label_left)\n",
    "            entropy_right = calculate_entropy(data_right, label_right)\n",
    "            info_gain = entropy_parent_node - (\n",
    "                ((len(data_left)/len(X)) * entropy_left) + \n",
    "                ((len(data_right)/len(X)) * entropy_right)\n",
    "            )\n",
    "            info_gain_dict[val] = info_gain\n",
    "#         print(\"+++-->>Inside calculate_info_gain() [CONTINUOUS] ... dictionary-len = \", \n",
    "#               len(info_gain_dict), \" ... printing info_gain_dict .. \", info_gain_dict)\n",
    "        return info_gain_dict\n",
    "    else:# CATEGORICAL\n",
    "        ## Partition into FOR EACH FEATURE\n",
    "#         entropy_per_val = {} # empty dictionary\n",
    "        num_examples_parent = len(X)\n",
    "        cumulative_entropy = 0.0 # cumulative entropy for all features\n",
    "        if num_examples_parent == 0: # SOMEHOW comes down to this\n",
    "            print(\"-->>Inside calculateInfoGain() .. num_examples_parent = \", num_examples_parent,\n",
    "                 \" returning 0\")\n",
    "            return 0\n",
    "        for val in unique_vals_features:\n",
    "            idx_equal_to_feature = X[:, feature_column] == val\n",
    "            data_of_feature = X[idx_equal_to_feature]\n",
    "            label_of_feature = Y[idx_equal_to_feature]\n",
    "            entropy_of_feature = calculate_entropy(data_of_feature, label_of_feature)\n",
    "#             print(\"val = \", val, \" , entropy of feature = \", entropy_of_feature)\n",
    "            proportion_of_examples_in_feature = float(len(data_of_feature)) / float(num_examples_parent)\n",
    "#             print(\"proportion of examples in feature = \", proportion_of_examples_in_feature)\n",
    "            cumulative_entropy = cumulative_entropy + (proportion_of_examples_in_feature * entropy_of_feature)\n",
    "#             print(\"cumulative entropy = \", cumulative_entropy, \" parent entropy = \", entropy_parent_node)\n",
    "        #             entropy_per_val[val] = entropy_of_feature\n",
    "        # now subtract from parent's entropy to return the information gain\n",
    "        info_gain = entropy_parent_node - cumulative_entropy\n",
    "        return info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing datasets\n",
    "#### Make the last column as 'Label' and rename it to 'Label'\n",
    "#### Make any string/object datatype to integers [encoding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess dataset 1 -> churn dataset\n",
    "###### 1. Tenure is continuous\n",
    "###### 2. MonthlyCharges is continuous\n",
    "###### 3. TotalCharges is continuous [object type ... changed to float type] [Some missing values ... spaces, delete those rows]\n",
    "###### 4. Drop customerID column\n",
    "Syntax: df.drop(df[df.score < 50].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_1(df):    \n",
    "    df_copy = df.copy(deep = True)\n",
    "    df_copy.rename(columns = {'Churn' : 'Label'}, inplace=True)\n",
    "    df_copy = df_copy.drop(\"customerID\", axis = 1)  # drop customer ID\n",
    "    df_copy.drop(df_copy[df_copy.TotalCharges == ' '].index, inplace=True)  # delete rows with spaces\n",
    "    df_copy[\"TotalCharges\"] = df_copy[\"TotalCharges\"].astype(float)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataset 3: All are continuous values, so we will binarize them\n",
    "###### Dataset 3: Drop the 'time' column, keep only 20,000 NO/False labels and keep ALL YES/True labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_3(df, data_size = 20000):\n",
    "    df.rename(columns={'Class':'Label'}, inplace=True)  \n",
    "    # drop 'Time' column\n",
    "    df = df.drop(\"Time\", axis = 1)\n",
    "    \n",
    "    df_yes = df[df['Label'] == 1]\n",
    "    df_no  = df[df['Label'] == 0]\n",
    "    \n",
    "    indices = df_no.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=data_size)  # only keeps 'k' amount of data\n",
    "    df_no_kept = df.loc[test_indices]\n",
    "#     df_no_dropped = df.drop(test_indices)\n",
    "    # recombine the 'YES' and 'NO' samples together into a new dataframe\n",
    "    df = pd.concat([df_yes, df_no_kept])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Make the 'Label' column as separate Labels and use other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
      "0  Female              0     Yes         No       1           No   \n",
      "1    Male              0      No         No      34          Yes   \n",
      "2    Male              0      No         No       2          Yes   \n",
      "3    Male              0      No         No      45           No   \n",
      "4  Female              0      No         No       2          Yes   \n",
      "\n",
      "      MultipleLines InternetService OnlineSecurity OnlineBackup  \\\n",
      "0  No phone service             DSL             No          Yes   \n",
      "1                No             DSL            Yes           No   \n",
      "2                No             DSL            Yes          Yes   \n",
      "3  No phone service             DSL            Yes           No   \n",
      "4                No     Fiber optic             No           No   \n",
      "\n",
      "  DeviceProtection TechSupport StreamingTV StreamingMovies        Contract  \\\n",
      "0               No          No          No              No  Month-to-month   \n",
      "1              Yes          No          No              No        One year   \n",
      "2               No          No          No              No  Month-to-month   \n",
      "3              Yes         Yes          No              No        One year   \n",
      "4               No          No          No              No  Month-to-month   \n",
      "\n",
      "  PaperlessBilling              PaymentMethod  MonthlyCharges  TotalCharges  \\\n",
      "0              Yes           Electronic check           29.85         29.85   \n",
      "1               No               Mailed check           56.95       1889.50   \n",
      "2              Yes               Mailed check           53.85        108.15   \n",
      "3               No  Bank transfer (automatic)           42.30       1840.75   \n",
      "4              Yes           Electronic check           70.70        151.65   \n",
      "\n",
      "  Label  \n",
      "0    No  \n",
      "1    No  \n",
      "2   Yes  \n",
      "3    No  \n",
      "4   Yes  \n"
     ]
    }
   ],
   "source": [
    "# data_frame = preprocess_dataset_3(data_frame_original)\n",
    "# print(data_frame.head(5))\n",
    "\n",
    "data_frame = preprocess_dataset_1(data_frame_original)\n",
    "print(data_frame.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: dataframe\n",
    "    Output: X, Y [numpy format]\n",
    "\"\"\"\n",
    "def separate_labels_and_features(df):\n",
    "    X = df.drop(\"Label\", axis = 1)\n",
    "    Y = df[\"Label\"]\n",
    "    return X.values, Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7032, 19)\n",
      "(7032,)\n",
      "[['Female' 0 'Yes' ... 'Electronic check' 29.85 29.85]\n",
      " ['Male' 0 'No' ... 'Mailed check' 56.95 1889.5]\n",
      " ['Male' 0 'No' ... 'Mailed check' 53.85 108.15]\n",
      " ...\n",
      " ['Female' 0 'Yes' ... 'Electronic check' 29.6 346.45]\n",
      " ['Male' 1 'Yes' ... 'Mailed check' 74.4 306.6]\n",
      " ['Male' 0 'No' ... 'Bank transfer (automatic)' 105.65 6844.5]]\n"
     ]
    }
   ],
   "source": [
    "X, Y = separate_labels_and_features(data_frame)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7032, 19)   7032\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, \" \", len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All functions below are to binarize the continuous values of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: Dataset (X, Y), feature-column\n",
    "    Output: Best Information Gain wrt THIS feature column\n",
    "    Dependency: Uses calculate_information_gain(X, Y, feature_column, feature_type) function\n",
    "                which returns the info_gain [if categorical]\n",
    "                which returns dictionary of {key = split_val, value = info_gain}\n",
    "\"\"\"\n",
    "def get_best_IG_val_of_this_feature(X_train, Y_train, feature_col, custom_col_list):\n",
    "#     print(X_train[:, 0])\n",
    "    dict_info_gain_col = calculate_information_gain(X_train, Y_train, \n",
    "                                      feature_col, feature_type=\"CONTINUOUS\",\n",
    "                                                   use_custom_columns=False)\n",
    "    print(\"-->Inside gt_best_IG_val_of_this_feature() .. col = \", feature_col ,\n",
    "          \" .. dictionary len = \", len(dict_info_gain_col))\n",
    "#     print(\"Printing dictionary ... \", dict_info_gain_col)\n",
    "#     dict_info_gain_col = calculate_information_gain(X, Y, feature_col, feature_type=\"CONTINUOUS\",\n",
    "#                               use_custom_columns = True, custom_columns_list = custom_col_list)  # WORKING\n",
    "    # https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "    v = list(dict_info_gain_col.values())\n",
    "    k = list(dict_info_gain_col.keys())\n",
    "    return k[v.index(max(v))], max(v)\n",
    "#     return dict_info_gain_col_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    To binarize the data, instead of comparing values for EACH continuous value,\n",
    "    we will divide into 100 (or user defined number of) data points, \n",
    "    and compare with each of those values. [To make it time-efficient (Takes ~ 2mins)]\n",
    "\"\"\"\n",
    "def get_dictionary_of_best_split_values_for_each_col(X, Y, num_values_of_custom_cols = 100,\n",
    "                        use_custom_col_list = False, custom_cols = None):\n",
    "    _, ncol = X.shape\n",
    "    num_cols_to_do = np.arange(ncol)\n",
    "    if use_custom_col_list == True:\n",
    "        num_cols_to_do = custom_cols\n",
    "    dict_best_ig_feature_split_values = {}  # Dictionary to store {feature_col: split_val, max_IG} that has max IG wrt that feature column\n",
    "#     for col_feature in range(ncol):  # for each feature_column\n",
    "    for col_feature in num_cols_to_do:\n",
    "        unique_vals_of_this_feature = np.unique(X[:, col_feature])\n",
    "        left_range = int(np.ceil(min(unique_vals_of_this_feature)))\n",
    "        right_range = int(np.floor(max(unique_vals_of_this_feature))) \n",
    "        cols_custom = np.linspace(left_range, right_range, num_values_of_custom_cols)\n",
    "        split_val_for_max_IG, max_IG = get_best_IG_val_of_this_feature(X, Y, col_feature, cols_custom)\n",
    "        dict_best_ig_feature_split_values[col_feature] = split_val_for_max_IG, max_IG\n",
    "    return dict_best_ig_feature_split_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Binarizes the dataset per column wrt one feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Actually binarizes the Data wrt the above dictionary found.\n",
    "\"\"\"\n",
    "def binarize_dataset(X, dict_split_values_and_max_IG_per_col):\n",
    "#     v = list(dict_split_values_and_max_IG_per_col.values())\n",
    "    X_bin = X\n",
    "    keys_list = list(dict_split_values_and_max_IG_per_col.keys())\n",
    "    features_column = {}\n",
    "#     print(keys_list)\n",
    "    for col in keys_list:\n",
    "        # dictionary[col][0] gives split-value and dictionary[col][1] gives max ig\n",
    "        split_value_of_col = dict_split_values_and_max_IG_per_col[col][0]\n",
    "        binarized_data_this_col = X[:, col] < split_value_of_col\n",
    "        X_bin[:, col] = binarized_data_this_col\n",
    "        features_column[col] = \"BINARIZED\"\n",
    "    \n",
    "    return X_bin, features_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_features_list(features_list_binarized, features_list_previous_arr):\n",
    "    features_list_new = {}\n",
    "    for itr in range(len(features_list_previous_arr)):\n",
    "        features_list_new[itr] = features_list_previous_arr[itr]\n",
    "    for key in list(features_list_binarized.keys()):\n",
    "        features_list_new[key] = features_list_binarized[key]\n",
    "    return list(features_list_new.values()) # returns as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Label encoding of labels/classes ...\n",
    "\"\"\"\n",
    "def label_encode_labels(Y):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(Y)\n",
    "    Y= le.transform(Y)\n",
    "    return Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Label encoding .... \n",
    "\"\"\"\n",
    "def label_encode_data(X, feature_types):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in range(len(feature_types)):\n",
    "        if feature_types[i] == \"CATEGORICAL\":\n",
    "            le.fit(X[:, i])\n",
    "            X[:, i] = le.transform(X[:, i])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CONTINUOUS', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CONTINUOUS', 'CONTINUOUS'] \n",
      " 19\n",
      "[4, 17, 18]\n",
      "================ BEFORE ======================\n",
      "-->Inside gt_best_IG_val_of_this_feature() .. col =  4  .. dictionary len =  72\n",
      "-->Inside gt_best_IG_val_of_this_feature() .. col =  17  .. dictionary len =  1584\n",
      "-->Inside gt_best_IG_val_of_this_feature() .. col =  18  .. dictionary len =  6530\n",
      "{4: (18, 0.07579675305132505), 17: (29.4, 0.03824153343031822), 18: (348.15, 0.034475641654172384)}\n",
      "-------------------- AFTER -------------------------\n",
      "['CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'BINARIZED', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'BINARIZED', 'BINARIZED']\n"
     ]
    }
   ],
   "source": [
    "df_2 = data_frame.drop(data_frame.columns.values[-1], axis=1)\n",
    "# print(df_2.head(2))\n",
    "feature_types_before = find_data_types(df_2)\n",
    "print(feature_types_before, \"\\n\", len(feature_types_before))\n",
    "indices_continous = []\n",
    "for i in range(len(feature_types_before)):\n",
    "    if feature_types_before[i] == \"CONTINUOUS\":\n",
    "        indices_continous.append(i)\n",
    "print(indices_continous)\n",
    "X, Y = separate_labels_and_features(data_frame)\n",
    "print(\"================ BEFORE ======================\")\n",
    "dict_test = get_dictionary_of_best_split_values_for_each_col(X, Y, 1000,\n",
    "                                                use_custom_col_list=True, custom_cols=indices_continous)\n",
    "print(dict_test)\n",
    "print(\"-------------------- AFTER -------------------------\")\n",
    "X_binarized, features_col_new = binarize_dataset(X, dict_test)\n",
    "features_col_new = get_new_features_list(features_col_new, feature_types_before)\n",
    "print(features_col_new)\n",
    "X_binarized = label_encode_data(X_binarized, features_col_new)\n",
    "Y = label_encode_labels(Y)\n",
    "# print(X_binarized)\n",
    "# Pass this binarized data into the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 ... 2 False True]\n",
      " [1 0 0 ... 3 False False]\n",
      " [1 0 0 ... 3 False True]\n",
      " ...\n",
      " [0 0 1 ... 2 False True]\n",
      " [1 1 1 ... 3 False True]\n",
      " [1 0 0 ... 0 False False]]\n",
      "[0 0 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Y = label_encode_labels(Y)\n",
    "print(X_binarized)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tenure   MonthlyCharges   TotalCharges\n"
     ]
    }
   ],
   "source": [
    "print(data_frame.columns[4], \" \", data_frame.columns[17], \" \", data_frame.columns[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col =  4  :-> Split-Val =  18  , InfoGain =  0.07579675305132505\n",
      "Col =  17  :-> Split-Val =  29.4  , InfoGain =  0.03824153343031822\n",
      "Col =  18  :-> Split-Val =  348.15  , InfoGain =  0.034475641654172384\n"
     ]
    }
   ],
   "source": [
    "for key in list(dict_test.keys()):\n",
    "    print(\"Col = \", key , \" :-> Split-Val = \", dict_test[key][0], \" , InfoGain = \", dict_test[key][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Compute new feature types ... [Binarized/Categorical/Continuous]\n",
    "\"\"\"\n",
    "def obtain_new_feature_types(feature_types_prev, dict_feature_types):  # WORKING\n",
    "    keys = list(dict_feature_types.keys())\n",
    "    feature_types_latest = feature_types_prev # Copy previous feature types ...\n",
    "    for i in keys:\n",
    "        feature_types_latest[i] = dict_feature_types[i]\n",
    "    return feature_types_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'BINARIZED', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'BINARIZED', 'BINARIZED']\n"
     ]
    }
   ],
   "source": [
    "# feature_types = obtain_new_feature_types(feature_types_before, features_col_new)\n",
    "# print(feature_types, \" \", len(feature_types))\n",
    "feature_types = features_col_new\n",
    "print(feature_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to numpy and train_test_split using scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_binarized, Y, test_size=0.2, random_state=0)\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X_binarized, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5625, 19)   (5625,)   (1407, 19)   (1407,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, \" \", Y_train.shape , \" \", X_test.shape, \" \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "19\n",
      "-----------------------------------------------\n",
      "(array([0, 1], dtype=object), array([2798, 2827], dtype=int64))\n",
      "(array([0, 1], dtype=object), array([685, 722], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1], dtype=object), array([4722,  903], dtype=int64))\n",
      "(array([0, 1], dtype=object), array([1168,  239], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1], dtype=object), array([2920, 2705], dtype=int64))\n",
      "(array([0, 1], dtype=object), array([719, 688], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1], dtype=object), array([3935, 1690], dtype=int64))\n",
      "(array([0, 1], dtype=object), array([998, 409], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([False, True], dtype=object), array([3524, 2101], dtype=int64))\n",
      "(array([False, True], dtype=object), array([882, 525], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1], dtype=object), array([ 550, 5075], dtype=int64))\n",
      "(array([0, 1], dtype=object), array([ 130, 1277], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1, 2], dtype=object), array([2703,  550, 2372], dtype=int64))\n",
      "(array([0, 1, 2], dtype=object), array([682, 130, 595], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1, 2], dtype=object), array([1932, 2471, 1222], dtype=int64))\n",
      "(array([0, 1, 2], dtype=object), array([484, 625, 298], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1, 2], dtype=object), array([2801, 1222, 1602], dtype=int64))\n",
      "(array([0, 1, 2], dtype=object), array([696, 298, 413], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1, 2], dtype=object), array([2476, 1222, 1927], dtype=int64))\n",
      "(array([0, 1, 2], dtype=object), array([611, 298, 498], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1, 2], dtype=object), array([2484, 1222, 1919], dtype=int64))\n",
      "(array([0, 1, 2], dtype=object), array([610, 298, 499], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1, 2], dtype=object), array([2786, 1222, 1617], dtype=int64))\n",
      "(array([0, 1, 2], dtype=object), array([686, 298, 423], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1, 2], dtype=object), array([2241, 1222, 2162], dtype=int64))\n",
      "(array([0, 1, 2], dtype=object), array([568, 298, 541], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1, 2], dtype=object), array([2213, 1222, 2190], dtype=int64))\n",
      "(array([0, 1, 2], dtype=object), array([568, 298, 541], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1, 2], dtype=object), array([3109, 1167, 1349], dtype=int64))\n",
      "(array([0, 1, 2], dtype=object), array([766, 305, 336], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1], dtype=object), array([2278, 3347], dtype=int64))\n",
      "(array([0, 1], dtype=object), array([586, 821], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([0, 1, 2, 3], dtype=object), array([1222, 1215, 1908, 1280], dtype=int64))\n",
      "(array([0, 1, 2, 3], dtype=object), array([320, 306, 457, 324], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([False, True], dtype=object), array([4329, 1296], dtype=int64))\n",
      "(array([False, True], dtype=object), array([1087,  320], dtype=int64))\n",
      "-----------------------------------------------\n",
      "(array([False, True], dtype=object), array([4332, 1293], dtype=int64))\n",
      "(array([False, True], dtype=object), array([1060,  347], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][0] == 0)\n",
    "print((X_train.shape[1]))\n",
    "for i in range((X_train.shape[1])):\n",
    "#     print(i, end=' ')\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(np.unique(X_train[:, i], return_counts = True))\n",
    "    print(np.unique(X_test[:, i], return_counts = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form decision tree bulding algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Returns the majority of the label\n",
    "\"\"\"\n",
    "def return_majority_label(labels):\n",
    "    # obtains unique labels AND also the counts of those unique labels\n",
    "    label_names, label_counts = np.unique(labels, return_counts = True)\n",
    "    index_max = label_counts.argmax()\n",
    "    labels_with_max_count = label_names[index_max]\n",
    "    return labels_with_max_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Class DecisionTreeNode to store the decision trees/subtrees nodes.\n",
    "    CAN'T HAVE CONTINUOUS DATA [ONLY BINARIZED/CATEGORICAL]\n",
    "\"\"\"\n",
    "class DTreeNode:\n",
    "    def __init__(self):\n",
    "        # self.sub_trees = []  # List of subtrees/children\n",
    "        # self.feature_values = [] # list of feature values of children [To ask question use indices]\n",
    "        self.children = {} # [feature_val: DTreeNode]\n",
    "        self.feature_col = -1 # which feature column THIS node contains as a question\n",
    "        self.is_leaf_node = False # by default, shouldn't be a leaf\n",
    "        self.classification = \"NONE\" # also consider as the plurality value\n",
    "        \n",
    "    def printTree(self, spaces_num = 0):\n",
    "        node = self\n",
    "        if node.is_leaf_node == True:  # Only print the classification column/feature\n",
    "            print(\" \" * spaces_num, \"Lab(\", node.classification, \")\")\n",
    "            return\n",
    "        else:  # Print the question\n",
    "            # print(\" \" * spaces_num, node.feature_col, \" \", node.get_comparison_mark(), \" \", node.feature_val)\n",
    "            print(\"\\n\", (\"    \" * spaces_num), \"Q(\", node.feature_col, \")\")\n",
    "        spaces_num = spaces_num + 1\n",
    "        for key in list(self.children.keys()):\n",
    "            print(\"  \"*spaces_num, \" == \", key)\n",
    "            node = self.children[key]\n",
    "            node.printTree(spaces_num)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Q( 0 )\n",
      "    ==  1\n",
      "  Lab( 0 )\n",
      "    ==  2\n",
      "  Lab( 1 )\n",
      "    ==  3\n",
      "\n",
      "      Q( 22 )\n",
      "      ==  13\n",
      "   Lab( 0 )\n",
      "      ==  31\n",
      "   Lab( 1 )\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Testing above data structure.\n",
    "\"\"\"\n",
    "root = DTreeNode()\n",
    "root.feature_col = 0\n",
    "child1 = DTreeNode()\n",
    "child1.is_leaf_node = True\n",
    "child1.classification = 0\n",
    "child2 = DTreeNode()\n",
    "child2.is_leaf_node = True\n",
    "child2.classification = 1\n",
    "child3 = DTreeNode()\n",
    "child3.feature_col = 22\n",
    "child31 = DTreeNode()\n",
    "child31.is_leaf_node = True\n",
    "child31.feature_col = 8\n",
    "child31.classification = 0\n",
    "child32 = DTreeNode()\n",
    "child32.is_leaf_node = True\n",
    "child32.feature_col = 10\n",
    "child32.classification = 1\n",
    "child3.children[13] = child31\n",
    "child3.children[31] = child32\n",
    "root.children[1] = child1\n",
    "root.children[2] = child2\n",
    "root.children[3] = child3\n",
    "root.printTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1   1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(return_majority_label([0, 0, 0, 1]))\n",
    "print(return_majority_label([0]))\n",
    "a = b = 1\n",
    "print(a, \" \", b)\n",
    "\n",
    "dicti = {1:\"A\", 2:\"B\", 3:\"C\", 4:\"D\"}\n",
    "print(list(dicti.keys())[0])\n",
    "# print(len(dicti.where(\"A\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Can't have CONTINUOUS data [Will be binarized first]\n",
    "    Decision Tree Classifier\n",
    "\"\"\"    \n",
    "class DTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.d_tree_root = DTreeNode()\n",
    "        self.max_depth = 5\n",
    "    \n",
    "    def form_a_leaf_node(self, labels):\n",
    "        leaf_node = DTreeNode()\n",
    "        leaf_node.is_leaf_node = True\n",
    "        leaf_node.classification = return_majority_label(labels)\n",
    "        return leaf_node\n",
    "    ##### Can't have CONTINUOUS data [ONLY CATEGORICAL/BINARIZED DATA IS ALLOWED !!]\n",
    "    def recursive_fit(self, X, Y, current_depth, max_depth):\n",
    "        ## 1. If current-depth == max-depth [Base Case]\n",
    "        if current_depth == max_depth:\n",
    "            return self.form_a_leaf_node(Y) # return the leaf node with majority of the current labels\n",
    "        ## 2. If EXACTLY one label ... return that\n",
    "        if (len(np.unique(Y)) == 1):\n",
    "            return self.form_a_leaf_node(Y) # return the leaf node with majority of the current labels\n",
    "        current_depth = current_depth + 1  # increment current_depth variable\n",
    "        val_max_IG = col_max_IG = -1\n",
    "        for col in range(0, X.shape[1]):  ## for each number of columns/features\n",
    "            ig_this_col = calculate_information_gain(X, Y, col, \"CATEGORICAL\")\n",
    "#             print(\"col = \", col, \" ig_col = \", ig_current_col)\n",
    "            if ig_this_col > val_max_IG:\n",
    "                val_max_IG = ig_this_col\n",
    "                col_max_IG = col\n",
    "        \n",
    "        ## 3. Max IG obtained is 0 [no more examples/features]\n",
    "        if val_max_IG == 0:\n",
    "            return self.form_a_leaf_node(Y)\n",
    "        \n",
    "        d_tree_node = DTreeNode()\n",
    "        d_tree_node.classification = return_majority_label(Y)  ### Saves the pluarility value of THIS node [used in prediction]\n",
    "        ## -> Recursion\n",
    "        for val_of_this_feature in np.unique(X[:, col_max_IG]):\n",
    "            index_child = (X[:, col_max_IG] == val_of_this_feature)\n",
    "            X_child = X[index_child]\n",
    "            Y_child = Y[index_child]\n",
    "\n",
    "            d_tree_node.is_leaf_node = False  ### is an internal node\n",
    "            d_tree_node.feature_col = col_max_IG  ### question is to be asked on this column/feature\n",
    "            # d_tree_node.feature_values.append(val_of_this_feature)\n",
    "            if (X_child.shape[0] == 0): # no more examples [DON'T return as other values of list will be empty]\n",
    "                d_tree_node.children.children[val_of_this_feature] = self.form_a_leaf_node(Y)  # return parent's max plurality value\n",
    "                # d_tree_node.sub_trees.append(self.form_a_leaf_node(labels))\n",
    "            else:\n",
    "            ##### DICTIONARY {key = feature_val, value = sub-tree}\n",
    "                sub_tree = self.recursive_fit(X_child, Y_child, current_depth, max_depth)\n",
    "                d_tree_node.children[val_of_this_feature] = sub_tree\n",
    "                # d_tree_node.sub_trees.append(sub_tree)\n",
    "        ### END OF FOR LOOP, return d_tree_node\n",
    "        return d_tree_node\n",
    "    \n",
    "    def printTree(self):\n",
    "        print(\"-->Inside printTree\")\n",
    "        self.d_tree_root.printTree()\n",
    "\n",
    "    def fit(self, examples, labels, max_depth = 5):\n",
    "        dt_node = self.recursive_fit(examples, labels, current_depth = 0, max_depth=max_depth)\n",
    "        self.d_tree_root = dt_node\n",
    "        print(\"Fit done for, max-depth = \", max_depth)\n",
    "        \n",
    "    def predict_one_example(self, example_to_predict):\n",
    "        self.d_tree_root_backup = self.d_tree_root\n",
    "        root = self.d_tree_root\n",
    "        if root == None:\n",
    "            raise Exception('Root of the Decision Tree is null !! [In predict()]')\n",
    "        while root is not None:\n",
    "            if root.is_leaf_node == True: # classify ... since, it is the leaf\n",
    "                self.d_tree_root = self.d_tree_root_backup\n",
    "                return root.classification\n",
    "            else:\n",
    "                col_to_process = root.feature_col\n",
    "                val_present_in_example = example_to_predict[col_to_process]\n",
    "                bool_found = False\n",
    "#                 print(\"In col = \", col_to_process, \" len root.children = \", len(root.children))\n",
    "                # for idx in range(len(root.feature_values)):\n",
    "                keys_feature_vals = list(root.children.keys())\n",
    "                for feature_val in keys_feature_vals:\n",
    "                    if example_to_predict[root.feature_col] == feature_val:\n",
    "                        root = root.children[feature_val]\n",
    "                        bool_found = True\n",
    "                        break\n",
    "                    \n",
    "                if bool_found == False: ### SHOULD RETURN PARENT's PLURALITY VALUE\n",
    "                    self.d_tree_root = self.d_tree_root_backup\n",
    "                    return root.classification # PARENT's plurality value\n",
    "                    #raise Exception(\"The value of \", val_present_in_example, \" doesn't exist for col_idx = \", col_to_process)\n",
    "                \n",
    "    def predict(self, examples_test):\n",
    "        labels = []\n",
    "        for example in examples_test:\n",
    "            yp1 = self.predict_one_example(example)\n",
    "            labels.append(yp1)\n",
    "        return labels\n",
    "    \n",
    "    \n",
    "    def print_metrics(self, Y_true, Y_predicted):\n",
    "        # print(len(Y_pred))\n",
    "        TN, FP, FN, TP = confusion_matrix(Y_true, Y_predicted).ravel()\n",
    "        accuracy = (TP + TN)/(TP+TN+FP+FN)\n",
    "        recall = (TP)/(TP + FN)\n",
    "        specificity = (TN)/(TN + FP)\n",
    "        precision = (TP)/(TP + FP)\n",
    "        false_discovery_rate = (FP)/(TP + FP)\n",
    "        f1_score = 2*((precision * recall) / (precision + recall))\n",
    "        print(\"TN = \", TN, \" FP = \", FP, \" FN = \", FN, \" TP = \", TP)\n",
    "        print(\"Accuracy = \", accuracy*100, \"%\")\n",
    "        print(\"TPR = Sensitivity = Recall = \", recall*100, \"%\")\n",
    "        print(\"Specificity = \", specificity*100, \"%\")\n",
    "        print(\"Precision = \", precision*100, \"%\")\n",
    "        print(\"FDR = False Discovery Rate = \", false_discovery_rate*100, \"%\")\n",
    "        print(\"F1 Score = \", f1_score*100, \"%\")\n",
    "        # return TN, FP, FN, TP, accuracy, recall, specificity, precision, false_discovery_rate, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit done for, max-depth =  10000\n"
     ]
    }
   ],
   "source": [
    "d_tree = DTreeClassifier()\n",
    "# d_tree.fit(X_train[0:100], Y_train[0:100], max_depth = 10000)\n",
    "d_tree.fit(X_train, Y_train, max_depth = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Training ----------------\n",
      "TN =  4060  FP =  65  FN =  240  TP =  1260\n",
      "Accuracy =  94.57777777777778 %\n",
      "TPR = Sensitivity = Recall =  84.0 %\n",
      "Specificity =  98.42424242424242 %\n",
      "Precision =  95.09433962264151 %\n",
      "FDR = False Discovery Rate =  4.905660377358491 %\n",
      "F1 Score =  89.20353982300884 %\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = d_tree.predict(X_train)\n",
    "print(\"------------- Training ----------------\")\n",
    "d_tree.print_metrics(Y_train, Y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Testing ----------------\n",
      "TN =  881  FP =  157  FN =  205  TP =  164\n",
      "Accuracy =  74.27149964463398 %\n",
      "TPR = Sensitivity = Recall =  44.44444444444444 %\n",
      "Specificity =  84.8747591522158 %\n",
      "Precision =  51.09034267912772 %\n",
      "FDR = False Discovery Rate =  48.90965732087228 %\n",
      "F1 Score =  47.53623188405797 %\n"
     ]
    }
   ],
   "source": [
    "Y_pred_test = d_tree.predict(X_test)\n",
    "print(\"------------- Testing ----------------\")\n",
    "d_tree.print_metrics(Y_test, Y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83      1038\n",
      "           1       0.35      0.08      0.14       369\n",
      "\n",
      "    accuracy                           0.72      1407\n",
      "   macro avg       0.55      0.51      0.48      1407\n",
      "weighted avg       0.64      0.72      0.65      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(criterion='entropy')\n",
    "dtc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== TRAINING =================\n",
      "TN =  4060  FP =  238  FN =  65  TP =  1262\n",
      "Accuracy =  94.61333333333334 %\n",
      "TPR = Sensitivity = Recall =  95.10173323285606 %\n",
      "Specificity =  94.46254071661238 %\n",
      "Precision =  84.13333333333334 %\n",
      "FDR = False Discovery Rate =  15.866666666666667 %\n",
      "F1 Score =  89.28192430137956 %\n",
      "+++++++++++++++ TESTING ++++++++++++++++++\n",
      "TN =  867  FP =  196  FN =  171  TP =  173\n",
      "Accuracy =  73.91613361762616 %\n",
      "TPR = Sensitivity = Recall =  50.2906976744186 %\n",
      "Specificity =  81.56161806208843 %\n",
      "Precision =  46.883468834688344 %\n",
      "FDR = False Discovery Rate =  53.116531165311656 %\n",
      "F1 Score =  48.527349228611506 %\n"
     ]
    }
   ],
   "source": [
    "Y_p_train = dtc.predict(X_train)\n",
    "Y_p_test = dtc.predict(X_test)\n",
    "print(\"=============== TRAINING =================\")\n",
    "d_tree.print_metrics(Y_p_train, Y_train)\n",
    "print(\"+++++++++++++++ TESTING ++++++++++++++++++\")\n",
    "d_tree.print_metrics(Y_p_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407\n",
      "------------------ Testing --------------------------\n",
      "TN =  860  FP =  178  FN =  193  TP =  176\n",
      "Accuracy =  73.6318407960199 %\n",
      "TPR = Sensitivity = Recall =  47.696476964769644 %\n",
      "Specificity =  82.85163776493256 %\n",
      "Precision =  49.717514124293785 %\n",
      "FDR = False Discovery Rate =  50.282485875706215 %\n",
      "F1 Score =  48.686030428769016 %\n"
     ]
    }
   ],
   "source": [
    "Y_pred_Test = dtc.predict(X_test)\n",
    "val = 1\n",
    "TN, FP, FN, TP, accuracy, recall, specificity, precision, false_discovery_rate, f1_score = calculate_metrics(val, X_test, Y_test, give_y_pred=True, y_pred_given=Y_pred_Test)\n",
    "print(\"------------------ Testing --------------------------\")\n",
    "print(\"TN = \", TN, \" FP = \", FP, \" FN = \", FN, \" TP = \", TP)\n",
    "print(\"Accuracy = \", accuracy*100, \"%\")\n",
    "print(\"TPR = Sensitivity = Recall = \", recall*100, \"%\")\n",
    "print(\"Specificity = \", specificity*100, \"%\")\n",
    "print(\"Precision = \", precision*100, \"%\")\n",
    "print(\"FDR = False Discovery Rate = \", false_discovery_rate*100, \"%\")\n",
    "print(\"F1 Score = \", f1_score*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]   [2 4 6]\n",
      "[ 2  8 18]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 4, 6])\n",
    "print(a, \" \", b)\n",
    "c = np.multiply(a, b)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
