{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1505022 [ML Offline 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_dataset_1 = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "file_name_dataset_2_train = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/adult.data\"\n",
    "file_name_dataset_2_test = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/adult.test\"\n",
    "file_name_dataset_3 = \"F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/creditcardfraud/creditcard.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: examples [numpy array, WITHOUT the labels], labels/classes [numpy array format]\n",
    "    Output: Entropy of THIS node\n",
    "    *** epsilon_small is used for log_2 operations (log2(0) may give unwanted exceptions)\n",
    "    Calculation: \n",
    "        for each label x of Labels:\n",
    "            probability[x] = x/num_examples\n",
    "        Entropy(node) = H(node) = - [ sum of probability[x]*log_2(probability[x]) ]\n",
    "\"\"\"\n",
    "def calculate_entropy(examples, labels, epsilon_small = 0.0000000000000000001): # WORKING\n",
    "    labels_unique = np.unique(labels) # obtain the unique labels of the data\n",
    "    # gives unique label_names, and counts for each unique label_names\n",
    "    label_names, label_counts = np.unique(labels,\n",
    "                                         return_counts = True) \n",
    "    label_probabilities = label_counts/sum(label_counts)\n",
    "    label_log_probabilities = np.log2((label_probabilities + epsilon_small))\n",
    "    label_products = label_probabilities * label_log_probabilities\n",
    "    entropy = -1 * sum(label_products)\n",
    "    if entropy == 0.0:  # to not return -0.0\n",
    "        entropy = 0.0\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Finds the feature types ...\n",
    "\"\"\"\n",
    "# If num of unique vals are greater than 20, declare as continuous [if float/int]\n",
    "def check_data_type_one(unique_vals, threshold_cnt):  \n",
    "    if len(unique_vals) == 0:\n",
    "        return \"CATEGORICAL\"  # just checking\n",
    "    sample_val = unique_vals[0]\n",
    "    if isinstance(sample_val, str):\n",
    "        return \"CATEGORICAL\"\n",
    "    if isinstance(sample_val, float):\n",
    "        return \"CONTINUOUS\"\n",
    "    if len(unique_vals) > threshold_cnt:\n",
    "        return \"CONTINUOUS\"\n",
    "    else:\n",
    "        return \"CATEGORICAL\"\n",
    "# If FLOAT continuous, ELSE if num of unique vals are greater than 20/threshold -> continuous\n",
    "def find_data_types(data_frame, threshold=20):\n",
    "    data_type_list = []\n",
    "    for feature_test in data_frame.columns.values:\n",
    "        val_first = data_frame[feature_test]\n",
    "        unique_vals = np.unique(data_frame[feature_test])\n",
    "        type_val = check_data_type_one(unique_vals, threshold)\n",
    "        data_type_list.append(type_val)\n",
    "        # Keep looping\n",
    "        \n",
    "    return data_type_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Information Gain Calculation wrt one feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input: X (examples), Y (labels), feature_column(idx on X), feature_type [CONTINUOUS/CATEGORICAL],\n",
    "            use_custom_columns {optional}, custom_columns_list {optional/=}\n",
    "    Output: Information Gain wrt that feature [CATEGORICAL/BINARIZED]\n",
    "            Dictionary of information gains wrt each values of that feature [CONTINUOUS]\n",
    "    Dependency: Uses calculate_entropy() function written above\n",
    "    Calculation:\n",
    "        for each value v of examples[feature_column]:\n",
    "            Partition new_examples[v] by choosing that feature.val == v [if categorical]\n",
    "            Partition new_examples[v] by choosing that feature.val <= v [if continuous]\n",
    "            calculate entropy of new_examples, H(S_feature_val_v)\n",
    "            calculate num_examples(S_feature_val_v)/num_examples(parent_node)\n",
    "            Use formula IG = H(S{parent}) - [Sum of |S_val|/|S| * H(S_val)]\n",
    "    Treat either as continuous, or categorical [binarized data is treated as categorical]\n",
    "\"\"\"\n",
    "def calculate_information_gain(X, Y, feature_column, feature_type,\n",
    "                              use_custom_columns = False, custom_columns_list = None):  # WORKING\n",
    "    entropy_parent_node = calculate_entropy(X, Y) # entropy of parent node\n",
    "    # Either use ALL values for this column OR use custom values only\n",
    "    if ((use_custom_columns == True)) :\n",
    "        unique_vals_features = custom_columns_list\n",
    "    else:\n",
    "        unique_vals_features = np.unique(X[:, feature_column])\n",
    "    if feature_type == \"CONTINUOUS\":\n",
    "        info_gain_dict = {}\n",
    "        for val in unique_vals_features:\n",
    "            idx_left_bool = X[:, feature_column] < float(val)\n",
    "            idx_right_bool = X[:, feature_column] >= float(val)\n",
    "            data_left = X[idx_left_bool]\n",
    "            label_left = Y[idx_left_bool]\n",
    "            data_right = X[idx_right_bool]\n",
    "            label_right = Y[idx_right_bool]\n",
    "            entropy_left = calculate_entropy(data_left, label_left)\n",
    "            entropy_right = calculate_entropy(data_right, label_right)\n",
    "            info_gain = entropy_parent_node - (\n",
    "                ((len(data_left)/len(X)) * entropy_left) + \n",
    "                ((len(data_right)/len(X)) * entropy_right)\n",
    "            )\n",
    "            info_gain_dict[val] = info_gain\n",
    "        return info_gain_dict\n",
    "    else:# CATEGORICAL\n",
    "        ## Partition into FOR EACH FEATURE\n",
    "        num_examples_parent = len(X)\n",
    "        cumulative_entropy = 0.0 # cumulative entropy for all features\n",
    "        if num_examples_parent == 0: # SOMEHOW comes down to this\n",
    "            print(\"-->>Inside calculateInfoGain() .. num_examples_parent = \", num_examples_parent,\n",
    "                 \" returning 0\")\n",
    "            return 0\n",
    "        for val in unique_vals_features:\n",
    "            idx_equal_to_feature = X[:, feature_column] == val\n",
    "            data_of_feature = X[idx_equal_to_feature]\n",
    "            label_of_feature = Y[idx_equal_to_feature]\n",
    "            entropy_of_feature = calculate_entropy(data_of_feature, label_of_feature)\n",
    "            proportion_of_examples_in_feature = float(len(data_of_feature)) / float(num_examples_parent)\n",
    "            cumulative_entropy = cumulative_entropy + (proportion_of_examples_in_feature * entropy_of_feature)\n",
    "        # now subtract from parent's entropy to return the information gain\n",
    "        info_gain = entropy_parent_node - cumulative_entropy\n",
    "        return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Obtains the best Info-Gain values of THIS GIVEN FEATURE COLUMN [can use custom columns list]\n",
    "    Input: Dataset (X, Y), feature-column, custom_columns [to NOT consider ALL values for splitting]\n",
    "    Output: Best Information Gain wrt THIS feature column\n",
    "    Dependency: Uses calculate_information_gain(X, Y, feature_column, feature_type) function\n",
    "                which returns the info_gain [if categorical]\n",
    "                which returns dictionary of {key = split_val, value = info_gain}\n",
    "\"\"\"\n",
    "def get_best_IG_val_of_this_feature(X_train, Y_train, feature_col, custom_col_list=None,\n",
    "                                   use_custom_col_list = False):\n",
    "    if use_custom_col_list == False:\n",
    "        dict_info_gain_col = calculate_information_gain(X_train, Y_train, \n",
    "                  feature_col, feature_type=\"CONTINUOUS\", use_custom_columns=False)\n",
    "    else:\n",
    "        dict_info_gain_col = calculate_information_gain(X_train, Y_train, \n",
    "  feature_col, feature_type=\"CONTINUOUS\", use_custom_columns=True, custom_columns_list=custom_col_list)\n",
    "    v = list(dict_info_gain_col.values())\n",
    "    k = list(dict_info_gain_col.keys())\n",
    "    return k[v.index(max(v))], max(v) # returns the max gain and index/split_val of that max IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    To binarize the data, instead of comparing values for EACH continuous value,\n",
    "    we will divide into 100 (or user defined number of) data points, \n",
    "    and compare with each of those values. [To make it time-efficient (Takes ~ 2mins)]\n",
    "\"\"\"\n",
    "def get_dictionary_of_best_split_values_for_each_col(X, Y, num_values_of_custom_cols = 100,\n",
    "                        use_custom_col_list = False, custom_cols = None, use_custom_end_points = False,\n",
    "                                                    which_cols_to_use_endpoints = None):\n",
    "    _, ncol = X.shape # Only need the no. of columns\n",
    "    num_cols_to_do = np.arange(ncol)\n",
    "    if use_custom_col_list == True:\n",
    "        num_cols_to_do = custom_cols\n",
    "    dict_best_ig_feature_split_values = {}  # Dictionary to store {feature_col: split_val, max_IG} that has max IG wrt that feature column \n",
    "    for idx_col in range(len(num_cols_to_do)): # EITHER ONLY DO FOR SELECTED COLUMNS/ OR FOR ALL COLUMNS\n",
    "        col_feature = num_cols_to_do[idx_col]\n",
    "        unique_vals_of_this_feature = np.unique(X[:, col_feature])\n",
    "        left_range = int(np.ceil(min(unique_vals_of_this_feature)))\n",
    "        right_range = int(np.floor(max(unique_vals_of_this_feature))) \n",
    "        cols_custom = np.linspace(left_range, right_range, num_values_of_custom_cols)\n",
    "        if use_custom_end_points == True:\n",
    "            if which_cols_to_use_endpoints is not None:\n",
    "                split_val_for_max_IG, max_IG = get_best_IG_val_of_this_feature(X, Y, col_feature, \n",
    "                                                       cols_custom, use_custom_col_list=True)\n",
    "            else:\n",
    "                if which_cols_to_use_endpoints[idx_col] == True:\n",
    "                    split_val_for_max_IG, max_IG = get_best_IG_val_of_this_feature(X, Y, col_feature, \n",
    "                               cols_custom, use_custom_col_list=True)\n",
    "        else:\n",
    "            # ELSE: Use ALL values for continuous ....\n",
    "            split_val_for_max_IG, max_IG = get_best_IG_val_of_this_feature(X, Y, col_feature)\n",
    "        dict_best_ig_feature_split_values[col_feature] = split_val_for_max_IG, max_IG\n",
    "        print(\"Dictionary: Done for Column = \", col_feature, \" split-val = \", split_val_for_max_IG, \" Max IG = \", max_IG)\n",
    "    return dict_best_ig_feature_split_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_features_list(features_list_binarized, features_list_previous_arr):\n",
    "    features_list_new = {}\n",
    "    for itr in range(len(features_list_previous_arr)):\n",
    "        features_list_new[itr] = features_list_previous_arr[itr]\n",
    "    for key in list(features_list_binarized.keys()):\n",
    "        features_list_new[key] = features_list_binarized[key]\n",
    "    return list(features_list_new.values()) # returns as list\n",
    "\"\"\"\n",
    "    Binarizes the Data wrt the above dictionary found.\n",
    "    Also, label encoding is done.\n",
    "\"\"\"\n",
    "def binarize_dataset(X, dict_split_values_and_max_IG_per_col):\n",
    "    X_bin = X\n",
    "    keys_list = list(dict_split_values_and_max_IG_per_col.keys())\n",
    "    features_column = {}\n",
    "    for col in keys_list:\n",
    "        # dictionary[col][0] gives split-value and dictionary[col][1] gives max IG.\n",
    "        split_value_of_col = dict_split_values_and_max_IG_per_col[col][0]\n",
    "        binarized_data_this_col = X[:, col] < split_value_of_col\n",
    "        X_bin[:, col] = binarized_data_this_col\n",
    "        features_column[col] = \"BINARIZED\"\n",
    "    \n",
    "    return X_bin, features_column\n",
    "\n",
    "def label_encode_labels(Y): # Label encoding of labels/classes/Y ...\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(Y)\n",
    "    Y= le.transform(Y)\n",
    "    return Y \n",
    "\n",
    "def label_encode_data(X, feature_types): # Label encoding of examples/X .... \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in range(len(feature_types)):\n",
    "        if feature_types[i] == \"CATEGORICAL\":\n",
    "            le.fit(X[:, i])\n",
    "            X[:, i] = le.transform(X[:, i])\n",
    "    return X\n",
    "\n",
    "\"\"\"\n",
    "    Input: dataframe\n",
    "    Output: X, Y [numpy format]\n",
    "\"\"\"\n",
    "def separate_labels_and_features(df):\n",
    "    X = df.drop(\"Label\", axis = 1)\n",
    "    Y = df[\"Label\"]\n",
    "    return X.values, Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset 1: churn dataset\n",
    "1. Tenure is continuous\n",
    "2. MonthlyCharges is continuous\n",
    "3. TotalCharges is continuous [object type ... changed to float type] [Some missing values ... spaces, delete those rows]\n",
    "4. Drop customerID column\n",
    "\"\"\"\n",
    "def preprocess_dataset_1(df):    \n",
    "    df_copy = df.copy(deep = True)\n",
    "    df_copy.rename(columns = {'Churn' : 'Label'}, inplace=True)\n",
    "    df_copy = df_copy.drop(\"customerID\", axis = 1)  # drop customer ID\n",
    "    df_copy.drop(df_copy[df_copy.TotalCharges == ' '].index, inplace=True)  # delete rows with spaces\n",
    "    df_copy[\"TotalCharges\"] = df_copy[\"TotalCharges\"].astype(float)\n",
    "    return df_copy\n",
    "\n",
    "\"\"\"\n",
    "Dataset 3: Credit-Card, all are continuous values, so we will binarize them\n",
    "Drop the 'time' column, keep only 20,000 NO/False labels and keep ALL YES/True labels\n",
    "\"\"\"\n",
    "def preprocess_dataset_3(df, data_size = 20000):\n",
    "    df.rename(columns={'Class':'Label'}, inplace=True)  \n",
    "    df = df.drop(\"Time\", axis = 1) # drop 'Time' column\n",
    "    df_yes = df[df['Label'] == 1]\n",
    "    df_no  = df[df['Label'] == 0]\n",
    "    indices = df_no.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=data_size)  # only keeps 'k' amount of data\n",
    "    df_no_kept = df.loc[test_indices]\n",
    "    df = pd.concat([df_yes, df_no_kept]) # recombine the 'YES' and 'NO' samples together into a new dataframe\n",
    "    return df\n",
    "\n",
    "def obtain_for_datasets_1_and_3(dataset_num = 1): # dataset 1 by default\n",
    "    \"\"\"\n",
    "        Datasets: 1 and 3 --> We will use this script\n",
    "    \"\"\"\n",
    "    print(\"Inside obtain_for_datasets_1_and_3() .. dataset_num = \", dataset_num)\n",
    "    if dataset_num == 1:\n",
    "        file_name_dataset = file_name_dataset_1\n",
    "        print(\"Reading from file: \", file_name_dataset)\n",
    "        data_frame_original = pd.read_csv(file_name_dataset) \n",
    "        print(data_frame_original.head(2))\n",
    "        data_frame = preprocess_dataset_1(data_frame_original)\n",
    "        print(data_frame.head(2))\n",
    "    else :\n",
    "        file_name_dataset = file_name_dataset_3\n",
    "        print(\"Reading from file: \", file_name_dataset)\n",
    "        data_frame_original = pd.read_csv(file_name_dataset) \n",
    "        print(data_frame_original.head(2))\n",
    "        data_frame = preprocess_dataset_3(data_frame_original)\n",
    "        print(data_frame.head(2))\n",
    "    \n",
    "    df_2 = data_frame.drop(data_frame.columns.values[-1], axis=1)\n",
    "    # print(df_2.head(2))\n",
    "    feature_types_before = find_data_types(df_2)\n",
    "    print(feature_types_before, \"\\n\", len(feature_types_before))\n",
    "    indices_continous = []\n",
    "    for i in range(len(feature_types_before)):  ## ONLY BINARIZE THE CONTINUOUS FEATURES ... [float/num unique values > 15]\n",
    "        if feature_types_before[i] == \"CONTINUOUS\":\n",
    "            indices_continous.append(i)\n",
    "    print(indices_continous)\n",
    "    X, Y = separate_labels_and_features(data_frame) # Separate the labels and features ...\n",
    "    print(\"================ BEFORE Binarization ======================\")\n",
    "    dict_split_vals_for_binarization = get_dictionary_of_best_split_values_for_each_col(X, Y, 100,\n",
    "                                            use_custom_col_list=True, custom_cols=indices_continous)\n",
    "\n",
    "    print(dict_split_vals_for_binarization)\n",
    "    print(\"-------------------- AFTER -------------------------\")\n",
    "    X_binarized, features_col_new = binarize_dataset(X, dict_split_vals_for_binarization)\n",
    "    features_col_new = get_new_features_list(features_col_new, feature_types_before)\n",
    "    X_binarized = label_encode_data(X_binarized, features_col_new)\n",
    "    Y = label_encode_labels(Y)\n",
    "    feature_types = features_col_new\n",
    "    print(feature_types)\n",
    "    # 20 % threshold, random_state = 8, train_test_split using scikitlearn\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_binarized, Y, test_size=0.2, random_state=8)\n",
    "    print(X_train.shape, \" \", Y_train.shape , \" \", X_test.shape, \" \", Y_test.shape)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "    # Pass this binarized data into the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset 2: Adult\n",
    "[Col, #Unique vals, #All vals]\n",
    "[ 0 ,  73 ,  32561 ] [ 2 ,  21648 ,  32561 ] [ 4 ,  16 ,  32561 ] \n",
    "[ 10 ,  119 ,  32561 ] [ 11 ,  92 ,  32561 ] [ 12 ,  94 ,  32561 ]\n",
    "col = 2 NEEDS to be discretized, otherwise will take too much time !!\n",
    "Problems: Rows contain '?', Continuous values for age, fnlwgt, education-num, capital-gain, capital-loss, hours-per-week\n",
    "Columns that have '?' marks are : workclass, occupation , native-country [we replace with MODE]\n",
    "Col =  workclass , Mode =   Private , Percent =  69.70301894904948  %\n",
    "Col =  occupation , Mode =   Prof-specialty  , Percent =  12.714597217530175  %\n",
    "Col =  native-country , Mode =   United-States  , Percent =  89.5857006848684  %\n",
    "\"\"\"\n",
    "def preprocess_dataset_2_train(df_original):\n",
    "    df = df_original.copy(deep = True)\n",
    "    print(\"No. of columns = \", len(df.columns.values))\n",
    "    df['education-num'] = df['education-num'].astype(float)  # to make THIS column float [so that it automatically becomes continuous]\n",
    "    list_columns_with_QUESTION_mark = []\n",
    "    for col in df.columns.values:\n",
    "        unique_vals, unique_counts = np.unique(df[col].values, return_counts=True)\n",
    "        missing_val = \" ?\"\n",
    "        if missing_val in unique_vals:  # ONLY THOSE COLUMNS THAT CONTAIN THE ' ?' mark\n",
    "            list_columns_with_QUESTION_mark.append(col)\n",
    "            mode_of_unique_vals = unique_vals[np.argmax(unique_counts)]\n",
    "#             print(\"Col = \", col, \" Mode = \", mode_of_unique_vals, \" Percent = \", (unique_counts[np.argmax(unique_counts)]/len(df))*100, \" %\")\n",
    "            df[col] = np.where((df[col] == ' ?'),mode_of_unique_vals,df[col])  # Replace '?' with mode of that column \n",
    "    return df, list_columns_with_QUESTION_mark\n",
    "\n",
    "def obtain_train_test_for_dataset_2():\n",
    "    column_names_dataset_2 = [\"age\", \"workclass\", \"fnlwgt\", \"education\",\n",
    "                       \"education-num\", \"marital-status\", \"occupation\",\n",
    "                       \"relationship\", \"race\", \"sex\", \"capital-gain\",\n",
    "                       \"capital-loss\", \"hours-per-week\", \"native-country\", \"Label\"]\n",
    "    for col in column_names_dataset_2:\n",
    "        print(col, end = ',')\n",
    "    print(\"-->> *** NEED to add the above list as HEADER\", col)\n",
    "    print(\"Train file: \", file_name_dataset_2_train, \" Test file: \", file_name_dataset_2_test)\n",
    "    data_frame_original = pd.read_csv(file_name_dataset_2_train)\n",
    "    data_frame, list_col_with_qstn_mark = preprocess_dataset_2_train(data_frame_original)\n",
    "    data_frame_test_original = pd.read_csv(file_name_dataset_2_test)\n",
    "    data_frame_test, list_col_with_qstn_mark_test = preprocess_dataset_2_train(data_frame_test_original)\n",
    "    X_train = (data_frame.drop(\"Label\", axis = 1)).values\n",
    "    Y_train = (data_frame[\"Label\"]).values\n",
    "    X_test = (data_frame_test.drop(\"Label\", axis = 1)).values\n",
    "    Y_test = (data_frame_test[\"Label\"]).values\n",
    "    print(\"Train: \", X_train.shape, \" \", Y_train.shape, \"  Test: \", X_test.shape, \" \", Y_test.shape)\n",
    "    \n",
    "    df_2 = data_frame.drop(data_frame.columns.values[-1], axis=1)\n",
    "    feature_types_before = find_data_types(df_2)\n",
    "    print(feature_types_before, \"\\n\", len(feature_types_before))\n",
    "    indices_continous = []\n",
    "    for i in range(len(feature_types_before)):  ## ONLY BINARIZE THE CONTINUOUS FEATURES ... [float/num unique values > 15]\n",
    "        print(df_2.columns.values[i], \" \", feature_types_before[i])\n",
    "        if feature_types_before[i] == \"CONTINUOUS\":\n",
    "            indices_continous.append(i)\n",
    "    print(indices_continous)\n",
    "    print(\"================ BEFORE Binarization ======================\")\n",
    "    indices_continous.remove(2) # REMOVE \"fnwlgt\" column [will discretize it later]\n",
    "    \n",
    "    dict_binarized_values = get_dictionary_of_best_split_values_for_each_col(X_train, Y_train, 100,\n",
    "                                use_custom_col_list=True, custom_cols=indices_continous,\n",
    "                                use_custom_end_points=False)\n",
    "\n",
    "    print(dict_binarized_values)\n",
    "    print(\"-------------------- AFTER -------------------------\")\n",
    "    X_binarized, features_col_new = binarize_dataset(X_train, dict_binarized_values) # binarize\n",
    "    features_col_new = get_new_features_list(features_col_new, feature_types_before)\n",
    "    X_train = label_encode_data(X_binarized, features_col_new) # label encoding [EXAMPLES, Training data]\n",
    "    enc = sklearn.preprocessing.KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans') # 10 bins\n",
    "    # strategies: quantile, kmeans, uniform [encod = 'ordinal' FOR integer]\n",
    "    X_train[:, 2] = (enc.fit_transform(X_train[:, 2].reshape(-1, 1))).reshape(-1, )\n",
    "    Y_train = label_encode_labels(Y_train) # label encoding [LABELS, Training data]\n",
    "    X_test, features_col_new_test = binarize_dataset(X_test, dict_binarized_values) # binarize\n",
    "    X_test = label_encode_data(X_test, features_col_new) # label encoding [EXAMPLES, TEST DATA]\n",
    "    X_test[:, 2] = (enc.fit_transform(X_test[:, 2].reshape(-1, 1))).reshape(-1, )  # DISCRETIZATION\n",
    "    Y_test = label_encode_labels(Y_test) # label encoding [LABELS, TEST DATA]\n",
    "    feature_types = features_col_new\n",
    "    print(feature_types)\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTreeNode and DTreeClassifier as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Class DTreeNode to store the decision trees/subtrees nodes.\n",
    "    CAN'T HAVE CONTINUOUS DATA [ONLY BINARIZED/CATEGORICAL(Pre-existing or Discretized)]\n",
    "\"\"\"\n",
    "class DTreeNode:\n",
    "    def __init__(self):\n",
    "        self.children = {} # [feature_val: DTreeNode]\n",
    "        self.feature_col = -1 # which feature column THIS node contains as a question\n",
    "        self.is_leaf_node = False # by default, shouldn't be a leaf\n",
    "        self.classification = \"NONE\" # also consider as the plurality value\n",
    "        \n",
    "    def printTree(self, spaces_num = 0):\n",
    "        node = self\n",
    "        if node.is_leaf_node == True:  # Only print the classification column/feature\n",
    "            print(\" \" * spaces_num, \"Lab(\", node.classification, \")\")\n",
    "            return\n",
    "        else:  # Print the question\n",
    "            print(\"\\n\", (\"    \" * spaces_num), \"Q(\", node.feature_col, \")\")\n",
    "        spaces_num = spaces_num + 1\n",
    "        for key in list(self.children.keys()):\n",
    "            print(\"  \"*spaces_num, \" == \", key)\n",
    "            node = self.children[key]\n",
    "            node.printTree(spaces_num)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Prints various metrics.\n",
    "\"\"\"\n",
    "def print_metrics(Y_true, Y_predicted):\n",
    "    TN, FP, FN, TP = confusion_matrix(Y_true, Y_predicted).ravel()\n",
    "    accuracy = (TP + TN)/(TP+TN+FP+FN)\n",
    "    recall = (TP)/(TP + FN)\n",
    "    specificity = (TN)/(TN + FP)\n",
    "    precision = (TP)/(TP + FP)\n",
    "    false_discovery_rate = (FP)/(TP + FP)\n",
    "    f1_score = 2*((precision * recall) / (precision + recall))\n",
    "    print(\"TN = \", TN, \" FP = \", FP, \" FN = \", FN, \" TP = \", TP)\n",
    "    print(\"Accuracy = \", accuracy*100, \"%\")\n",
    "    print(\"TPR = Sensitivity = Recall = \", recall*100, \"%\")\n",
    "    print(\"TNR = Specificity = \", specificity*100, \"%\")\n",
    "    print(\"Precision = PPV = Positive Predictive Value = \", precision*100, \"%\")\n",
    "    print(\"FDR = False Discovery Rate = \", false_discovery_rate*100, \"%\")\n",
    "    print(\"F1 Score = \", f1_score*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Can't have CONTINUOUS data [SHOULD be binarized/discretized first]\n",
    "    Decision Tree Classifier\n",
    "\"\"\"\n",
    "def return_majority_label(labels): # Returns the majority of the label\n",
    "    # obtains unique labels AND also the counts of those unique labels\n",
    "    label_names, label_counts = np.unique(labels, return_counts = True)\n",
    "    index_max = label_counts.argmax()\n",
    "    labels_with_max_count = label_names[index_max]\n",
    "    return labels_with_max_count\n",
    "\n",
    "class DTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.d_tree_root = DTreeNode()\n",
    "        self.max_depth = 5  # default max-depth = 5\n",
    "        self.print_termination_msg = True\n",
    "    \n",
    "    def form_a_leaf_node(self, labels):\n",
    "        leaf_node = DTreeNode()\n",
    "        leaf_node.is_leaf_node = True\n",
    "        leaf_node.classification = return_majority_label(labels)\n",
    "        return leaf_node\n",
    "    ##### Can't have CONTINUOUS data [ONLY CATEGORICAL/BINARIZED DATA IS ALLOWED !!]\n",
    "    def recursive_fit(self, X, Y, X_parent, Y_parent, current_depth, max_depth):\n",
    "        ## 1. If current-depth == max-depth [Base Case]\n",
    "        if current_depth == max_depth:\n",
    "            return self.form_a_leaf_node(Y) # return the leaf node with majority of the current labels\n",
    "        ## 2. If EXACTLY one label ... return that\n",
    "        if (len(np.unique(Y)) == 1):\n",
    "            return self.form_a_leaf_node(Y) # return the leaf node with majority of the current labels\n",
    "        ## 2_2. If no more examples, return max label of parent\n",
    "        if len(X) == 0:\n",
    "            return self.form_a_leaf_node(Y_parent)\n",
    "        \n",
    "        current_depth = current_depth + 1  # increment current_depth variable\n",
    "        val_max_IG = col_max_IG = -1\n",
    "        for col in range(0, X.shape[1]):  ## for each number of columns/features\n",
    "            ig_this_col = calculate_information_gain(X, Y, col, \"CATEGORICAL\") # DOESN't use feature_types_array\n",
    "#             print(\"col = \", col, \" ig_col = \", ig_current_col)\n",
    "            if ig_this_col > val_max_IG:\n",
    "                val_max_IG = ig_this_col\n",
    "                col_max_IG = col\n",
    "        \n",
    "        ## 3. Max IG obtained is 0 [no more examples/features]\n",
    "        if val_max_IG == 0:\n",
    "            return self.form_a_leaf_node(Y)\n",
    "        \n",
    "        d_tree_node = DTreeNode()\n",
    "        d_tree_node.classification = return_majority_label(Y)  ### Saves the pluarility value of THIS node [used in prediction]\n",
    "        ## -> Recursion\n",
    "        for val_of_this_feature in np.unique(X[:, col_max_IG]):\n",
    "            index_child = (X[:, col_max_IG] == val_of_this_feature)\n",
    "            X_child = X[index_child]\n",
    "            Y_child = Y[index_child]\n",
    "            d_tree_node.is_leaf_node = False  ### is an internal node\n",
    "            d_tree_node.feature_col = col_max_IG  ### question is to be asked on this column/feature\n",
    "            sub_tree = self.recursive_fit(X_child, Y_child, X, Y, current_depth, max_depth)\n",
    "            d_tree_node.children[val_of_this_feature] = sub_tree\n",
    "        ### END OF FOR LOOP, return d_tree_node\n",
    "        return d_tree_node\n",
    "    \n",
    "    def printTree(self):\n",
    "        self.d_tree_root.printTree()\n",
    "\n",
    "    def fit(self, examples, labels): # FIT FUNCTION [calls recursive_fit function above using suitable params]\n",
    "        dt_node = self.recursive_fit(examples, labels, examples, labels,\n",
    "                                     current_depth = 0, max_depth=self.max_depth)\n",
    "        self.d_tree_root = dt_node\n",
    "        if self.print_termination_msg == True:\n",
    "            print(\"Fit done for, max-depth = \", self.max_depth)\n",
    "        \n",
    "    def predict_one_example(self, example_to_predict):\n",
    "        self.d_tree_root_backup = self.d_tree_root\n",
    "        root = self.d_tree_root\n",
    "        if root == None:\n",
    "            raise Exception('Root of the Decision Tree is null !! [In predict()]')\n",
    "        while root is not None:\n",
    "            if root.is_leaf_node == True: # classify ... since, it is the leaf\n",
    "                self.d_tree_root = self.d_tree_root_backup\n",
    "                return root.classification\n",
    "            else:\n",
    "                col_to_process = root.feature_col\n",
    "                val_present_in_example = example_to_predict[col_to_process]\n",
    "                bool_found = False\n",
    "                keys_feature_vals = list(root.children.keys())\n",
    "                for feature_val in keys_feature_vals:\n",
    "                    if example_to_predict[root.feature_col] == feature_val:\n",
    "                        root = root.children[feature_val]\n",
    "                        bool_found = True\n",
    "                        break\n",
    "                    \n",
    "                if bool_found == False: ### SHOULD RETURN PARENT's PLURALITY VALUE\n",
    "                    self.d_tree_root = self.d_tree_root_backup\n",
    "                    return root.classification # PARENT's plurality value\n",
    "                    #raise Exception(\"The value of \", val_present_in_example, \" doesn't exist for col_idx = \", col_to_process)\n",
    "                \n",
    "    def predict(self, examples_test):\n",
    "        labels = []\n",
    "        for example in examples_test:\n",
    "            yp1 = self.predict_one_example(example)\n",
    "            labels.append(yp1)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age,workclass,fnlwgt,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,hours-per-week,native-country,Label,-->> *** NEED to add the above list as HEADER Label\n",
      "Train file:  F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/adult.data  Test file:  F:/Programs C and Java/Sessional Things/Assignments-Github/ML Assignments/adult.test\n",
      "No. of columns =  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-369-d9fd1860c63c>:21: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if missing_val in unique_vals:  # ONLY THOSE COLUMNS THAT CONTAIN THE ' ?' mark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of columns =  15\n",
      "Train:  (32561, 14)   (32561,)   Test:  (16281, 14)   (16281,)\n",
      "['CONTINUOUS', 'CATEGORICAL', 'CONTINUOUS', 'CATEGORICAL', 'CONTINUOUS', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CONTINUOUS', 'CONTINUOUS', 'CONTINUOUS', 'CATEGORICAL'] \n",
      " 14\n",
      "age   CONTINUOUS\n",
      "workclass   CATEGORICAL\n",
      "fnlwgt   CONTINUOUS\n",
      "education   CATEGORICAL\n",
      "education-num   CONTINUOUS\n",
      "marital-status   CATEGORICAL\n",
      "occupation   CATEGORICAL\n",
      "relationship   CATEGORICAL\n",
      "race   CATEGORICAL\n",
      "sex   CATEGORICAL\n",
      "capital-gain   CONTINUOUS\n",
      "capital-loss   CONTINUOUS\n",
      "hours-per-week   CONTINUOUS\n",
      "native-country   CATEGORICAL\n",
      "[0, 2, 4, 10, 11, 12]\n",
      "================ BEFORE Binarization ======================\n",
      "Dictionary: Done for Column =  0  split-val =  28  Max IG =  0.0737162839458736\n",
      "Dictionary: Done for Column =  4  split-val =  13.0  Max IG =  0.07069368427626688\n",
      "Dictionary: Done for Column =  10  split-val =  7298  Max IG =  0.08695927355015609\n",
      "Dictionary: Done for Column =  11  split-val =  1825  Max IG =  0.023190335847712906\n",
      "Dictionary: Done for Column =  12  split-val =  42  Max IG =  0.040778829211068346\n",
      "{0: (28, 0.0737162839458736), 4: (13.0, 0.07069368427626688), 10: (7298, 0.08695927355015609), 11: (1825, 0.023190335847712906), 12: (42, 0.040778829211068346)}\n",
      "-------------------- AFTER -------------------------\n",
      "['BINARIZED', 'CATEGORICAL', 'CONTINUOUS', 'CATEGORICAL', 'BINARIZED', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'CATEGORICAL', 'BINARIZED', 'BINARIZED', 'BINARIZED', 'CATEGORICAL']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Obtain X_train, Y_train, X_test, Y_test wrt which dataset.\n",
    "\"\"\"\n",
    "# X_train, Y_train, X_test, Y_test = obtain_for_datasets_1_and_3(dataset_num=1)\n",
    "X_train, Y_train, X_test, Y_test = obtain_train_test_for_dataset_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script to train and predict using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit done for, max-depth =  10000\n",
      "------------- Training ----------------\n",
      "TN =  23995  FP =  725  FN =  1826  TP =  6015\n",
      "Accuracy =  92.16547403335278 %\n",
      "TPR = Sensitivity = Recall =  76.71215406198189 %\n",
      "TNR = Specificity =  97.06715210355988 %\n",
      "Precision = PPV = Positive Predictive Value =  89.2433234421365 %\n",
      "FDR = False Discovery Rate =  10.756676557863502 %\n",
      "F1 Score =  82.50462931211851 %\n",
      "------------- Testing ----------------\n",
      "TN =  11359  FP =  1076  FN =  1617  TP =  2229\n",
      "Accuracy =  83.45924697500153 %\n",
      "TPR = Sensitivity = Recall =  57.956318252730114 %\n",
      "TNR = Specificity =  91.3470044229996 %\n",
      "Precision = PPV = Positive Predictive Value =  67.44326777609683 %\n",
      "FDR = False Discovery Rate =  32.556732223903175 %\n",
      "F1 Score =  62.34093133827437 %\n",
      "\n",
      "\n",
      "Total time taken =  14  s\n"
     ]
    }
   ],
   "source": [
    "t_before = datetime.datetime.now()\n",
    "d_tree_custom_DT = DTreeClassifier()\n",
    "# d_tree.fit(X_train[0:100], Y_train[0:100], max_depth = 10000)\n",
    "d_tree_custom_DT.max_depth = 10000 # here, depth-max is not to be considered ...\n",
    "d_tree_custom_DT.fit(X_train, Y_train)\n",
    "Y_pred_train = d_tree_custom_DT.predict(X_train)\n",
    "print(\"------------- Training ----------------\")\n",
    "print_metrics(Y_train, Y_pred_train)\n",
    "Y_pred_test = d_tree_custom_DT.predict(X_test)\n",
    "print(\"------------- Testing ----------------\")\n",
    "t_after = datetime.datetime.now()\n",
    "del_t = t_after - t_before\n",
    "print_metrics(Y_test, Y_pred_test)\n",
    "\n",
    "print(\"\\n\\nTotal time taken = \", del_t.seconds, \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AdaBoost Class:\n",
    "h: contains the K number of classifiers\n",
    "z: contains the weights of each classifier\n",
    "K : number of classifiers   \n",
    "w, a vector of 'N' weights, 1/N initially\n",
    "L_weak = self.produce_classifier_generic() # generic interface [in this case a decision stump]\n",
    "\"\"\"\n",
    "class AdaBoost:\n",
    "    def __init__(self, mode=\"DECISION_STUMP\"):\n",
    "        self.mode = mode\n",
    "    \n",
    "    # factory method to produce classifier\n",
    "    def produce_classifier_generic(self): # for now, default is decision stump\n",
    "        if self.mode == \"DECISION_STUMP\":\n",
    "            classifier = DTreeClassifier()\n",
    "            classifier.print_termination_msg = False\n",
    "            classifier.max_depth = 1 # decision stump\n",
    "        elif self.mode == \"SKLEARN\":\n",
    "            classifier = DecisionTreeClassifier(criterion='entropy', max_depth = 1) # DOESN'T work\n",
    "        else:  # Other modes depending on the 'mode' variable\n",
    "            classifier = None\n",
    "        return classifier\n",
    "    \n",
    "    \n",
    "    def normalize(self, w):\n",
    "        if sum(w) == 0:\n",
    "            return w\n",
    "        else:\n",
    "            return w/sum(w)\n",
    "    \n",
    "    def resample(self, examples, labels, w):\n",
    "        # returns resampled data and labels according to weights\n",
    "        w = self.normalize(w)\n",
    "        resampled_data_size = len(labels)\n",
    "        temp_arr = np.arange(resampled_data_size)\n",
    "        indices_sampled = np.random.choice(temp_arr, len(temp_arr), p=w)  # N resampled data points for now\n",
    "        examples_sampled = []\n",
    "        labels_sampled = []\n",
    "#         print(\"Inside resample ... indices_sampled = \", indices_sampled)\n",
    "        for idx in indices_sampled:  # probably a better method exists using numpy\n",
    "            examples_sampled.append(examples[idx])\n",
    "            labels_sampled.append(labels[idx])\n",
    "        \n",
    "        return np.asarray(examples_sampled), np.asarray(labels_sampled)\n",
    "    \n",
    "    # Return the label with the maximum weight of classifier\n",
    "    def predict(self, example):\n",
    "        labels_hypothesis = np.zeros(self.K) # Initially set to zeros\n",
    "        unique_labs_weights = {}\n",
    "        for k in range(self.K):\n",
    "            labels_hypothesis[k] = self.h[k].predict_one_example(example)            \n",
    "        for unique_labels in (np.unique(labels_hypothesis)):\n",
    "            unique_labs_weights[unique_labels] = 0.0 # initially set as 0 weight\n",
    "        for k in range(self.K):\n",
    "            unique_labs_weights[labels_hypothesis[k]] = unique_labs_weights[labels_hypothesis[k]] + self.z[k] \n",
    "        \n",
    "        v = list(unique_labs_weights.values())\n",
    "        k = list(unique_labs_weights.keys())\n",
    "        return k[v.index(max(v))]  # return the unique label with the MAX weight\n",
    "        \n",
    "        \n",
    "    def boost(self, examples, labels, K):     \n",
    "        self.K = K\n",
    "        N = examples.shape[0]\n",
    "        self.w = np.full(shape=(N, ), fill_value = (1/N))\n",
    "        self.h = {} # dictionary\n",
    "        self.z = np.zeros(shape=(K, ))\n",
    "        for k in range(K):\n",
    "            data_resampled, labels_resampled = self.resample(examples, labels, self.w)\n",
    "#             print(\"-> k = \", k, \" w = \", self.w, \"\\nlabels_counts = \", np.unique(labels_resampled, return_counts=True))\n",
    "            self.h[k] = self.produce_classifier_generic()\n",
    "            # FIT on RESAMPLED data [labels are also resampled]\n",
    "            self.h[k].fit(data_resampled, labels_resampled) \n",
    "#             h[k] = L_weak\n",
    "            error = 0\n",
    "            for j in range(0, N):\n",
    "                if self.h[k].predict_one_example(examples[j]) != labels[j]: # PREDICT on INITIAL EXAMPLES\n",
    "                    error = error + self.w[j]\n",
    "#             print(\"-->>k = \", k, \", Error = \", error, \"\\n\\n\")\n",
    "            if error > 0.5:\n",
    "                continue\n",
    "            for j in range(0, N):\n",
    "                if self.h[k].predict_one_example(examples[j]) == labels[j]:\n",
    "                    self.w[j] = self.w[j] * (error/(1 - error))  # error CAN'T be 1 due to continue condition\n",
    "            \n",
    "            # Normalize w\n",
    "            self.w = self.normalize(self.w)\n",
    "            # Error can be 0\n",
    "            if error == 0:\n",
    "                self.z[k] = 10 # max of 10^10 ?? [impossible case so, assign a large number]\n",
    "            else:\n",
    "                self.z[k] = np.log((1-error)/error)\n",
    "        # No need to return, since h,z are going to be stored \n",
    "#         return self.Weighted_Majority(h, z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Train AdaBoost K =  5 -----------------\n",
      "TN =  23495  FP =  1225  FN =  4634  TP =  3207\n",
      "Accuracy =  82.00608089432143 %\n",
      "TPR = Sensitivity = Recall =  40.90039535773498 %\n",
      "TNR = Specificity =  95.04449838187702 %\n",
      "Precision = PPV = Positive Predictive Value =  72.36010830324909 %\n",
      "FDR = False Discovery Rate =  27.6398916967509 %\n",
      "F1 Score =  52.2610608653141 %\n",
      "------------- Test AdaBoost K =  5 -----------------\n",
      "TN =  11814  FP =  621  FN =  2281  TP =  1565\n",
      "Accuracy =  82.17554204287207 %\n",
      "TPR = Sensitivity = Recall =  40.69162766510661 %\n",
      "TNR = Specificity =  95.00603136308806 %\n",
      "Precision = PPV = Positive Predictive Value =  71.59194876486734 %\n",
      "FDR = False Discovery Rate =  28.408051235132664 %\n",
      "F1 Score =  51.88992042440318 %\n",
      "------------- Train AdaBoost K =  10 -----------------\n",
      "TN =  23732  FP =  988  FN =  4223  TP =  3618\n",
      "Accuracy =  83.99619176315224 %\n",
      "TPR = Sensitivity = Recall =  46.14207371508736 %\n",
      "TNR = Specificity =  96.0032362459547 %\n",
      "Precision = PPV = Positive Predictive Value =  78.5497177594442 %\n",
      "FDR = False Discovery Rate =  21.450282240555797 %\n",
      "F1 Score =  58.13449023861171 %\n",
      "------------- Test AdaBoost K =  10 -----------------\n",
      "TN =  11935  FP =  500  FN =  2095  TP =  1751\n",
      "Accuracy =  84.06117560346415 %\n",
      "TPR = Sensitivity = Recall =  45.527821112844514 %\n",
      "TNR = Specificity =  95.97909127462808 %\n",
      "Precision = PPV = Positive Predictive Value =  77.78764993336294 %\n",
      "FDR = False Discovery Rate =  22.21235006663705 %\n",
      "F1 Score =  57.43808430375594 %\n",
      "------------- Train AdaBoost K =  15 -----------------\n",
      "TN =  23676  FP =  1044  FN =  4040  TP =  3801\n",
      "Accuracy =  84.3862289241731 %\n",
      "TPR = Sensitivity = Recall =  48.475959699017984 %\n",
      "TNR = Specificity =  95.7766990291262 %\n",
      "Precision = PPV = Positive Predictive Value =  78.45201238390092 %\n",
      "FDR = False Discovery Rate =  21.54798761609907 %\n",
      "F1 Score =  59.92432602869305 %\n",
      "------------- Test AdaBoost K =  15 -----------------\n",
      "TN =  11905  FP =  530  FN =  2001  TP =  1845\n",
      "Accuracy =  84.45427185062343 %\n",
      "TPR = Sensitivity = Recall =  47.97191887675507 %\n",
      "TNR = Specificity =  95.73783675110576 %\n",
      "Precision = PPV = Positive Predictive Value =  77.6842105263158 %\n",
      "FDR = False Discovery Rate =  22.315789473684212 %\n",
      "F1 Score =  59.31522263301719 %\n",
      "------------- Train AdaBoost K =  20 -----------------\n",
      "TN =  23439  FP =  1281  FN =  3698  TP =  4143\n",
      "Accuracy =  84.70870059273365 %\n",
      "TPR = Sensitivity = Recall =  52.83764825915062 %\n",
      "TNR = Specificity =  94.81796116504854 %\n",
      "Precision = PPV = Positive Predictive Value =  76.38274336283186 %\n",
      "FDR = False Discovery Rate =  23.61725663716814 %\n",
      "F1 Score =  62.46513381078025 %\n",
      "------------- Test AdaBoost K =  20 -----------------\n",
      "TN =  11797  FP =  638  FN =  1818  TP =  2028\n",
      "Accuracy =  84.91493151526319 %\n",
      "TPR = Sensitivity = Recall =  52.73010920436817 %\n",
      "TNR = Specificity =  94.8693204664254 %\n",
      "Precision = PPV = Positive Predictive Value =  76.06901725431358 %\n",
      "FDR = False Discovery Rate =  23.93098274568642 %\n",
      "F1 Score =  62.28501228501228 %\n",
      "Total time taken =  59  s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AdaBoost: running script\n",
    "\"\"\"\n",
    "k_list = [5, 10, 15, 20]\n",
    "# k_list = [5]\n",
    "t_before = datetime.datetime.now()\n",
    "for K in k_list:\n",
    "    adaBoost = AdaBoost()\n",
    "#     adaBoost = AdaBoost(\"SKLEARN\") # To check\n",
    "    adaBoost.boost(X_train, Y_train, K)\n",
    "    Y_pred_train_adaBoost = []\n",
    "    for ex in X_train:  # predict train dataset\n",
    "        Y_pred_train_adaBoost.append(adaBoost.predict(ex))\n",
    "    Y_pred_test_adaBoost = []\n",
    "    for ex in X_test:  # predict test dataset\n",
    "        Y_pred_test_adaBoost.append(adaBoost.predict(ex))\n",
    "    print(\"------------- Train AdaBoost K = \", K, \"-----------------\")\n",
    "    print_metrics(Y_train, Y_pred_train_adaBoost)\n",
    "    print(\"------------- Test AdaBoost K = \", K, \"-----------------\")\n",
    "    print_metrics(Y_test, Y_pred_test_adaBoost)\n",
    "    \n",
    "t_after = datetime.datetime.now()\n",
    "del_t = t_after - t_before\n",
    "print(\"Total time taken = \", del_t.seconds, \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== SKLEARN TRAINING [to compare]=================\n",
      "TN =  4051  FP =  231  FN =  58  TP =  1285\n",
      "Accuracy =  94.86222222222223 %\n",
      "TPR = Sensitivity = Recall =  95.6813104988831 %\n",
      "TNR = Specificity =  94.60532461466605 %\n",
      "Precision = PPV = Positive Predictive Value =  84.76253298153034 %\n",
      "FDR = False Discovery Rate =  15.237467018469658 %\n",
      "F1 Score =  89.89157047918853 %\n",
      "+++++++++++++++ SKLEARN TESTING [to compare]++++++++++++++++++\n",
      "TN =  875  FP =  198  FN =  179  TP =  155\n",
      "Accuracy =  73.20540156361052 %\n",
      "TPR = Sensitivity = Recall =  46.40718562874252 %\n",
      "TNR = Specificity =  81.54706430568498 %\n",
      "Precision = PPV = Positive Predictive Value =  43.90934844192634 %\n",
      "FDR = False Discovery Rate =  56.09065155807366 %\n",
      "F1 Score =  45.12372634643376 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    To compare using Sklearn's Decision Tree Classifier\n",
    "\"\"\"\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(criterion='entropy')\n",
    "dtc.fit(X_train, Y_train)\n",
    "Y_p_train = dtc.predict(X_train)\n",
    "Y_p_test = dtc.predict(X_test)\n",
    "print(\"=============== SKLEARN TRAINING [to compare]=================\")\n",
    "print_metrics(Y_p_train, Y_train)\n",
    "print(\"+++++++++++++++ SKLEARN TESTING [to compare]++++++++++++++++++\")\n",
    "print_metrics(Y_p_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
